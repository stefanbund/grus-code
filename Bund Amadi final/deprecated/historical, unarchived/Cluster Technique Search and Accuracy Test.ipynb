{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7c5d04-6844-47bc-b7ab-15572b05d1d2",
   "metadata": {},
   "source": [
    "## Cluster Technique Search and Accuracy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6873776c-9e64-4e97-83ba-a1c93e578d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "\n",
    "#SPECIFIED CLUSTER DISCOVERY\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering, Birch\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering,\\\n",
    "MeanShift, AffinityPropagation, DBSCAN, OPTICS, Birch\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "440ccac3-a41f-4b7d-acba-1c429cc35a00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "precursor_buy_cap_pct_change    float64\n",
       "precursor_ask_cap_pct_change    float64\n",
       "precursor_bid_vol_pct_change    float64\n",
       "precursor_ask_vol_pct_change    float64\n",
       "change.1                        float64\n",
       "surge_targets_met_pct           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BIN TECHNIQUE\n",
    "# load pipeline 1 csv and prep for clustering\n",
    "m2_pipeline = pd.read_csv('pipeline1.csv')\n",
    "# change is surge price rate of change per observation, change.1 is precursor\n",
    "# sum_change is surge sum_change per surge, and surge_area is surge alone\n",
    "keepable = ['precursor_buy_cap_pct_change', \n",
    "            'precursor_ask_cap_pct_change',\n",
    "            'precursor_bid_vol_pct_change', \n",
    "            'precursor_ask_vol_pct_change', 'change.1',\n",
    "            'surge_targets_met_pct']\n",
    "\n",
    "# Normalize the 'surge_targets_met_pct' column\n",
    "x = m2_pipeline[['surge_targets_met_pct']].values.astype(float)\n",
    "m2_pipeline = m2_pipeline[keepable]\n",
    "print(m2_pipeline.isna().sum(axis=1).astype(bool).sum())\n",
    "m2_pipeline = m2_pipeline.astype('float')\n",
    "m2_pipeline.dtypes\n",
    "\n",
    "# # bins = [float(f\"{x:.2f}\") for x in range(-10, 11)]\n",
    "# bins = [x * 0.2 for x in range(-10, 11)]\n",
    "# print(bins)\n",
    "# #model \n",
    "# m2_pipeline['bin'] = pd.cut(m2_pipeline['surge_targets_met_pct'], bins=bins  )#, labels=labels)\n",
    "# # Display the binned data\n",
    "# print(m2_pipeline['bin'].value_counts())\n",
    "# m2_pipeline['bin']\n",
    "# m2_pipeline['bin'] = m2_pipeline['bin'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85aeb500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-4.24232081911261,\n",
       " -2.6666666666666665,\n",
       " -1.3333333333333333,\n",
       " 0,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 1,\n",
       " 2,\n",
       " 5.322377307519139]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins = [\n",
    "    m2_pipeline['surge_targets_met_pct'].min() -1,  # Min value\n",
    "    -4/3*2,  # -4 to 0 divided into three equal parts\n",
    "    -4/3,  # Second bin edge for negative values\n",
    "    0,  # Zero separating negative and positive values\n",
    "    0.25, 0.5, 0.75, 1,  # Four bins between 0 and 1\n",
    "    2,  # One bin between 1 and 2\n",
    "    m2_pipeline['surge_targets_met_pct'].max() + 1]\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f8d6dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_pipeline['label'] = pd.cut(m2_pipeline['surge_targets_met_pct'], bins=bins, labels=list(range(1, len(bins))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "024682c2-746e-4162-9453-13016958db4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['precursor_buy_cap_pct_change', 'precursor_ask_cap_pct_change',\n",
       "       'precursor_bid_vol_pct_change', 'precursor_ask_vol_pct_change',\n",
       "       'change.1', 'surge_targets_met_pct', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2_pipeline.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5405cb9f-cfa2-44ee-bcf8-845a5bfd9a05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m2_pipeline.to_csv('binned_pipeline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ad79207-adab-4e2e-a5e9-473111635457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_test_results = [] #capture each test summary here, make df on this later\n",
    "profitability_analytics =[] #capture the value of each cluster, bin x price avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5efd8bd4-7b2d-4ea3-b849-b4f5e00df1fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cluster_profit(cluster):\n",
    "    print(cluster) #method, cluster count, silhouette score\n",
    "    # for the dataframe m2_pipeline, group by 'cluster' then 'bin', multiply bin as number by the average price by bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5f4762-7027-4dca-879a-ab1bf88dc9a5",
   "metadata": {},
   "source": [
    "### standardize all features pre train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c9affa-6ded-45cc-89c5-d9949ae4e904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['bin'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m m2_pipeline \u001b[38;5;241m=\u001b[39m m2_pipeline\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Splitting the dataframe into features and labels\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m X \u001b[38;5;241m=\u001b[39m m2_pipeline\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbin\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m m2_pipeline[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbin\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Performing the test/train split\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m   5400\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   5401\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   5402\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   5403\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   5404\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   5405\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[1;32m   5406\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   5407\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['bin'] not found in axis\""
     ]
    }
   ],
   "source": [
    "\n",
    "m2_pipeline = m2_pipeline.dropna()\n",
    "\n",
    "# Splitting the dataframe into features and labels\n",
    "X = m2_pipeline.drop(columns=['bin'])\n",
    "y = m2_pipeline['bin']\n",
    "\n",
    "# Performing the test/train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "#DO STANDARDIZATION ONCE YOU DO TEST TRAIN NOT BEFORE \n",
    "def silhouette_scorer(estimator, X, y=None):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    return score\n",
    "# Defining the parameter grid for GridSearchCV\n",
    "param_grid = {'n_clusters': [2,3,4,5,6,7,8,9,10]}  #'algorithm': ['auto', 'full', 'elkan']\n",
    "\n",
    "clustering_models = [\n",
    "    ('KMeans', KMeans()),\n",
    "    # ('SpectralClustering', SpectralClustering(eigen_solver=None, n_components=None, random_state=42, n_init=10, gamma=1.0, affinity='rbf',\\\n",
    "    #                   n_neighbors=10, eigen_tol='auto', assign_labels='kmeans', degree=3, coef0=1, kernel_params=None, n_jobs=None, verbose=False)),\n",
    "    ('Birch', Birch()),\n",
    "    ('Hierarchical',AgglomerativeClustering())]\n",
    "# Performing GridSearchCV for each clustering model\n",
    "for model_name, model in clustering_models:\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring=silhouette_scorer)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    # Evaluating the best model based on silhouette score\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(f'{model_name}:')\n",
    "    print(f'Best parameters: {grid_search.best_params_}')\n",
    "    print(f'Silhouette score (train): {best_score:.4f}')\n",
    "    print(f'Silhouette score (test): {best_score:.4f}')\n",
    "    best_params = grid_search.best_params_\n",
    "    big_dict = grid_search.cv_results_\n",
    "    pkg = {\"algo\": model_name, \"best_params\": grid_search.best_params_,\n",
    "        \"best_estimator\": grid_search.best_estimator_,\n",
    "        \"best_score\": best_score, \"all_results\": big_dict} \n",
    "    # # Add the cluster label as a new column\n",
    "    method_df = X_train_scaled #as it stands, then append a new column to it, write to csv\n",
    "    method_df['cluster_label'] = best_model.labels_\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    print(\"Current date and time: \")\n",
    "    print(now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    run_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    file_name = 'clustered/'+ 'clustered_'+ model_name+' _'+ run_time+ '.csv'\n",
    "    print(file_name)\n",
    "    method_df.to_csv(file_name)\n",
    "    # print(pkg)\n",
    "    global_test_results.append(pkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06622dc5-9e4d-4430-8fbe-15cfc8986f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering, Birch\n",
    "from sklearn.cluster import MeanShift, AffinityPropagation, DBSCAN, OPTICS\n",
    "from sklearn.metrics import silhouette_score\n",
    "# Splitting the dataframe into features and labels\n",
    "X = m2_pipeline.drop(columns=['bin'])\n",
    "y = m2_pipeline['bin']\n",
    "# Performing the test/train split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "def silhouette_scorer(estimator, X, y=None):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    return score\n",
    "\n",
    "param_grid = {}  #'algorithm': ['auto', 'full', 'elkan']\n",
    "clustering_models = [\n",
    "    ('Meanshift',  MeanShift( bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300)),\n",
    "    ('AffinityPropagation', AffinityPropagation( damping=0.5, max_iter=200, convergence_iter=15, copy=True, preference=None, affinity='euclidean', verbose=False, random_state=42)),\n",
    "    ('DBSCAN', DBSCAN(eps=0.5,  min_samples=5, metric='euclidean', metric_params=None, algorithm='auto', leaf_size=30, p=None, n_jobs=None)),\n",
    "('OPTICS', OPTICS( min_samples=5, max_eps=3, metric='minkowski', p=2, metric_params=None, cluster_method='xi', eps=None, xi=0.05, predecessor_correction=True, min_cluster_size=None, algorithm='auto', leaf_size=30, memory=None, n_jobs=None))]\n",
    "# Performing GridSearchCV for each clustering model\n",
    "for model_name, model in clustering_models:\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring=silhouette_scorer)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    # Evaluating the best model based on silhouette score\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(f'{model_name}:')\n",
    "    # print(f'Best parameters: {grid_search.best_params_}')\n",
    "    print(f'Silhouette score (train): {best_score:.4f}')\n",
    "    print(f'Silhouette score (test): {best_score:.4f}')\n",
    "    big_dict = grid_search.cv_results_\n",
    "    pkg = {\"algo\":model_name, \"best_params\":grid_search.best_params_, \"best_estimator\":grid_search.best_estimator_,\\\n",
    "            \"best_score\":best_score, \"all_results\":big_dict}\n",
    "    # print(pkg)\n",
    "    global_test_results.append(pkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb955e5-3d93-4da6-8eba-95a76f5e451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import SpectralClustering\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_clusters': [2,3,4,5,6,7,8,9,10],\n",
    "#     'affinity': ['nearest_neighbors', 'rbf'],\n",
    "#     'gamma': [0.1, 1.0, 10.0]\n",
    "# }\n",
    "# def silhouette_scorer(estimator, X, y=None):\n",
    "#     labels = estimator.fit_predict(X)\n",
    "#     score = silhouette_score(X, labels)\n",
    "#     return score\n",
    "# # ('SpectralClustering', SpectralClustering(eigen_solver=None, n_components=None, random_state=42, n_init=10, gamma=1.0, affinity='rbf',\\\n",
    "#     #                   n_neighbors=10, eigen_tol='auto', assign_labels='kmeans', degree=3, coef0=1, kernel_params=None, n_jobs=None, verbose=False)),\n",
    "# clustering = SpectralClustering(eigen_solver=None, n_components=None, random_state=42, n_init=10, gamma=1.0, affinity='rbf',\\\n",
    "#                       n_neighbors=10, eigen_tol='auto', assign_labels='kmeans', degree=3, coef0=1, kernel_params=None, n_jobs=None, verbose=False)\n",
    "# grid_search = GridSearchCV(clustering, param_grid, scoring=silhouette_scorer)\n",
    "# grid_search.fit(X_train_scaled, y_train)\n",
    "# # Evaluating the best model based on silhouette score\n",
    "# best_model = grid_search.best_estimator_\n",
    "# best_score = grid_search.best_score_\n",
    "# print(f'{model_name}:')\n",
    "# # print(f'Best parameters: {grid_search.best_params_}')\n",
    "# print(f'Silhouette score (train): {best_score:.4f}')\n",
    "# print(f'Silhouette score (test): {best_score:.4f}')\n",
    "# big_dict = grid_search.cv_results_\n",
    "# pkg = {\"algo\":model_name, \"best_params\":grid_search.best_params_, \"best_estimator\":grid_search.best_estimator_,\\\n",
    "#         \"best_score\":best_score, \"all_results\":big_dict}\n",
    "# # print(pkg)\n",
    "# global_test_results.append(pkg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5d5dec-84e5-426f-a7e8-c78d01d952dc",
   "metadata": {},
   "source": [
    "## explore clustering methods by profitability\n",
    "\n",
    "apply the cluster id to the dataframe, group by, then summarize value by bin x price range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09f98f9-92de-453e-bf8d-72961c698a2d",
   "metadata": {},
   "source": [
    "## explore top results by cluster quality scoring\n",
    "\n",
    "use silhouette viz\n",
    "\n",
    "then davies\n",
    "\n",
    "then kolmogorov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b625c5d-576e-4e10-a2db-7c4e2c9ff563",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(global_test_results)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd096ad6-dba8-4226-be34-4002f520c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#charting silhouettes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "# Calculate the silhouette scores for each sample\n",
    "silhouette_values = silhouette_samples(X, labels)\n",
    "\n",
    "# Calculate the average silhouette score\n",
    "average_score = silhouette_score(X, labels)\n",
    "\n",
    "# Plot the silhouette chart\n",
    "fig, ax = plt.subplots()\n",
    "y_lower = 10\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    # Aggregate the silhouette scores for samples in cluster i and sort them\n",
    "    ith_cluster_silhouette_values = silhouette_values[labels == i]\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    # Fill the silhouette chart with the corresponding color\n",
    "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "    # Label each cluster with its silhouette score\n",
    "    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "    \n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "ax.set_xlabel(\"Silhouette coefficient values\")\n",
    "ax.set_ylabel(\"Cluster label\")\n",
    "\n",
    "# The vertical line indicates the average silhouette score\n",
    "ax.axvline(x=average_score, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c7312-02c2-4b6b-af2f-9f044b677acb",
   "metadata": {},
   "source": [
    "## top method hyperparameter optimization\n",
    "take the top technique by silhouette then dive into a fuller exploration of its specific hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987147d-516e-4e6d-8990-59f2d27a2379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    'n_clusters': [2, 3, 4],\n",
    "    'affinity': ['nearest_neighbors', 'rbf'],\n",
    "    'gamma': [0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "clustering = SpectralClustering()\n",
    "random_search = RandomizedSearchCV(clustering, param_distributions)\n",
    "random_search.fit(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b99ad-37c5-4d5f-8007-5bffa28473c7",
   "metadata": {},
   "source": [
    "## estimate the model accuracy, givne best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8d6b0-fe5b-401a-ba7f-2f54e479f8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#use best model:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import accuracy_score\n",
    "X = m2_pipeline.drop(columns=['bin'])\n",
    "y = m2_pipeline['bin']\n",
    "# Splitting the dataset into train and test sets\n",
    "\n",
    "#USE STANDARD SCALING ONCE YOU DIVDE TEST AND TRAIN NOT BEFORE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating an instance of SpectralClustering\n",
    "model = SpectralClustering(n_clusters=7)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "model.fit_predict(X_train)\n",
    "\n",
    "# Predicting labels for the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
