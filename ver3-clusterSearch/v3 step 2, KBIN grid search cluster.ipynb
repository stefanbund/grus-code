{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aaa4fcc-1a03-4f20-a9dd-7ce3d60ccb52",
   "metadata": {},
   "source": [
    "## premise: Grid search for clustering solution\n",
    "\n",
    "via skleanr pipeline and grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beea8fea-ae81-4e3f-9bff-fc4f8bd3f443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171fd8a-33e5-4bb3-9d60-745ee812506d",
   "metadata": {},
   "source": [
    "## using a discretizer on our target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0084e4ae-20f9-4081-93df-7822264899b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "precursor_buy_cap_pct_change    float64\n",
       "precursor_ask_cap_pct_change    float64\n",
       "precursor_bid_vol_pct_change    float64\n",
       "precursor_ask_vol_pct_change    float64\n",
       "change.1                        float64\n",
       "surge_targets_met_pct           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load pipeline 1 csv and prep for clustering\n",
    "m2_pipeline = pd.read_csv('pipeline1.csv')\n",
    "#change is surge price rate of change per observation, change.1 is precursor\n",
    "#sum_change is surge sum_change per surge, and surge_area is surge alone\n",
    "keepable = ['precursor_buy_cap_pct_change', \n",
    "            'precursor_ask_cap_pct_change',\n",
    "            'precursor_bid_vol_pct_change', \n",
    "            'precursor_ask_vol_pct_change', 'change.1',\n",
    "            'surge_targets_met_pct']\n",
    "\n",
    "# Normalize the 'surge_targets_met_pct' column\n",
    "x = m2_pipeline[['surge_targets_met_pct']].values.astype(float)\n",
    "m2_pipeline = m2_pipeline[keepable]\n",
    "m2_pipeline = m2_pipeline.dropna()\n",
    "print(m2_pipeline.isna().sum(axis=1).astype(bool).sum())\n",
    "m2_pipeline = m2_pipeline.astype('float')\n",
    "m2_pipeline.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973b9b8c-19a3-4ca5-8db0-363fe6714b5a",
   "metadata": {},
   "source": [
    "## using predefined bins for discrete clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee7267ce-8674-4724-bb80-0c7b19ec41c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.    -8.95  -7.89  -6.84  -5.79  -4.74  -3.68  -2.63  -1.58  -0.53\n",
      "   0.53   1.58   2.63   3.68   4.74   5.79   6.84   7.89   8.95  10.  ]\n"
     ]
    }
   ],
   "source": [
    "# Define the maximum and minimum values\n",
    "maximum = 10\n",
    "minimum = -10\n",
    "\n",
    "# Create a range of 20 values between the maximum and minimum\n",
    "values = np.linspace(minimum, maximum, num=20)\n",
    "\n",
    "# Round each value to two decimal places\n",
    "rounded_values = np.round(values, decimals=2)\n",
    "\n",
    "# Print the resulting array\n",
    "print(rounded_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "433b59a8-d42b-4de7-84d6-aca0bc6e5fe0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input array must be 1 dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# m2_pipeline['binned'] = pd.cut(m2_pipeline['surge_targets_met_pct'], bins=bins  )#, labels=labels)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Display the binned data\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# print(m2_pipeline['binned'].value_counts())\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#implement\u001b[39;00m\n\u001b[1;32m     12\u001b[0m transformer \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mFunctionTransformer(\n\u001b[1;32m     13\u001b[0m     pd\u001b[38;5;241m.\u001b[39mcut, kw_args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbins\u001b[39m\u001b[38;5;124m'\u001b[39m: bins, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretbins\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m#'labels': labels, \u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GRUS/caret1/lib/python3.8/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/GRUS/caret1/lib/python3.8/site-packages/sklearn/base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/Desktop/GRUS/caret1/lib/python3.8/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/GRUS/caret1/lib/python3.8/site-packages/sklearn/preprocessing/_function_transformer.py:238\u001b[0m, in \u001b[0;36mFunctionTransformer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform X using the forward function.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m    Transformed input.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_input(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkw_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GRUS/caret1/lib/python3.8/site-packages/sklearn/preprocessing/_function_transformer.py:310\u001b[0m, in \u001b[0;36mFunctionTransformer._transform\u001b[0;34m(self, X, func, kw_args)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m     func \u001b[38;5;241m=\u001b[39m _identity\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkw_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkw_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GRUS/caret1/lib/python3.8/site-packages/pandas/core/reshape/tile.py:242\u001b[0m, in \u001b[0;36mcut\u001b[0;34m(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# NOTE: this binning code is changed a bit from histogram for var(x) == 0\u001b[39;00m\n\u001b[1;32m    241\u001b[0m original \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m--> 242\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43m_preprocess_for_cut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m x, dtype \u001b[38;5;241m=\u001b[39m _coerce_to_type(x)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39miterable(bins):\n",
      "File \u001b[0;32m~/Desktop/GRUS/caret1/lib/python3.8/site-packages/pandas/core/reshape/tile.py:602\u001b[0m, in \u001b[0;36m_preprocess_for_cut\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    600\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput array must be 1 dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mValueError\u001b[0m: Input array must be 1 dimensional"
     ]
    }
   ],
   "source": [
    "#model \n",
    "\n",
    "# Bin the values in the 'surge_targets_met_pct' column\n",
    "# bins = [-float('inf'), -8.5, -5.64, 0, 0.25, 0.35, 0.4, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0, 5.0, 5.64, float('inf')]\n",
    "bins = rounded_values\n",
    "labels = rounded_values #['< -8.5', '-8.5 to -5.64', '-5.64 to 0', '0 to 0.25', '0.25 to 0.35', '0.35 to 0.4', '0.4 to 0.5', '0.5 to 0.75', '0.75 to 1', '1 to 2', '2 to 3', '3 to 4', '4 to 5', '5 to 5.64', '> 5.64']\n",
    "# m2_pipeline['binned'] = pd.cut(m2_pipeline['surge_targets_met_pct'], bins=bins  )#, labels=labels)\n",
    "\n",
    "# Display the binned data\n",
    "# print(m2_pipeline['binned'].value_counts())\n",
    "#implement\n",
    "transformer = preprocessing.FunctionTransformer(\n",
    "    pd.cut, kw_args={'bins': bins, 'retbins': False}  #'labels': labels, \n",
    ")\n",
    "transformer.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d454ab-b649-4abd-bce4-801893ab1ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c1315-37ff-4616-83a6-6954b77456a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_pipeline['binned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150bccbc-b02a-4d47-829a-09e13fbc488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## you must cluster on the column 'surge_targets_met_pct' like so\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "m2_pipeline['surge_targets_met_pct_normalized'] = pd.DataFrame(x_scaled)\n",
    "\n",
    "# Assuming your dataframe is named 'm2_pipeline'\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "kmeans.fit(m2_pipeline[['surge_targets_met_pct_normalized']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c089cf4d-7ce1-4e9d-90cc-200e5e874716",
   "metadata": {},
   "source": [
    "### clustering types\n",
    "\n",
    "(https://scikit-learn.org/stable/modules/clustering.html#mean-shift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432f579-c5e4-4cfb-b90d-5208ebd28da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## alternative: specifying cluster value\n",
    "import pandas as pd \n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering,\\\n",
    "MeanShift, AffinityPropagation, DBSCAN, OPTICS, Birch\n",
    "from sklearn.metrics import silhouette_score \n",
    "\n",
    "data =  m2_pipeline\n",
    "scores = []  # feed in experiments as a dictionary of parameters and scores\n",
    "\n",
    "csize = [4,5,6,7,8,9,10, 11, 12]\n",
    "for i in range(len(csize)):\n",
    "    c= csize[i]\n",
    "    print(\"for cluster size\",c)\n",
    "    kmeans = KMeans(n_clusters=c) #\n",
    "    hierarchical = AgglomerativeClustering(n_clusters=c) \n",
    "    spectral = SpectralClustering(n_clusters=c) \n",
    "    birch = Birch(n_clusters=c)\n",
    "    algorithms = [(kmeans, 'KMeans'), (hierarchical, 'Agglomerative'), (spectral, 'Spectral'), (birch, 'Birch')] \n",
    "    # algorithms = [kmeans, hierarchical, spectral, birch] \n",
    "\n",
    "    # for algorithm, name in algorithms: # Loop over the algorithms and calculate the silhouette score for each\n",
    "    for algorithm, name in algorithms: # Loop over the algorithms and calculate the silhouette score for each\n",
    "        algorithm.fit(data) \n",
    "        score= silhouette_score(data, algorithm.labels_) \n",
    "        fin = {\"algo\":name, \"score\":score, \"clusters\":c}\n",
    "        print(fin)\n",
    "        scores.append(fin) \n",
    "\n",
    "results = pd.DataFrame(scores)  #{'Algorithm': [name for _, name in algorithms], 'Silhouette Score': scores}) \n",
    "print(results) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ac6ec6-07ca-4446-a171-7514528aae03",
   "metadata": {},
   "source": [
    "## do non cluster specified clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b488d-2a38-4a29-9db2-0ff80367ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanshift = MeanShift(*, bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300)\n",
    "affinity = AffinityPropagation(*, damping=0.5, max_iter=200, convergence_iter=15, copy=True, preference=None, affinity='euclidean', verbose=False, random_state=42)\n",
    "dbscan = DBSCAN(eps=0.5, *, min_samples=5, metric='euclidean', metric_params=None, algorithm='auto', leaf_size=30, p=None, n_jobs=None)\n",
    "optics = OPTICS(*, min_samples=5, max_eps=inf, metric='minkowski', p=2, metric_params=None, cluster_method='xi', eps=None, xi=0.05, predecessor_correction=True, min_cluster_size=None, algorithm='auto', leaf_size=30, memory=None, n_jobs=None)\n",
    "\n",
    "data = m2_pipeline\n",
    "# scores2 = []  # feed in experiments as a dictionary of parameters and scores\n",
    "algorithms2 = [(meanshift, 'Meanshift'), (affinity, 'Affinity'), (dbscan, 'DBSCAN'), (optics, 'OPTICS')]\n",
    "for algorithm, name in algorithms2: # Loop over the algorithms and calculate the silhouette score for each\n",
    "        algorithm.fit(data) \n",
    "        score= silhouette_score(data, algorithm.labels_) \n",
    "        fin = {\"algo\":name, \"score\":score, \"clusters\":0} \n",
    "        print(fin)\n",
    "        scores.append(fin) \n",
    "# Create a dataframe to store the results \n",
    "results = pd.DataFrame(scores) #{'Algorithm': [name for _, name in algorithms2], 'Silhouette Score': scores2}) \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c956a-97b0-4aea-ab44-bb91c5861e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = 'ranked_classifier_search.csv'  #clustered methods, specified num clusters\n",
    "results.to_csv(cm, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24043d0-1bb8-4920-8bd2-1443fc526324",
   "metadata": {},
   "source": [
    "## revive the test from stored memory\n",
    "via csv, then get top n, n=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e1de45-14ca-42e8-9f87-c25f49cb418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "revive = pd.read_csv(cm)\n",
    "print(revive.head(2))\n",
    "print(revive.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f88d16b-5d01-4f3a-a230-af262364ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = revive.sort_values('score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d05c5-9316-4b1e-964f-3aae0bdc43e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5588d8-7f9a-4db4-981c-08bbf65a7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topn.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9438c5-46cc-4392-884e-3039e9845cf5",
   "metadata": {},
   "source": [
    "## cluster and back test the cluster as trade strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26324684-6a33-4962-a32b-73a46e4b6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cluster range evaluation\n",
    "# Bin the values in the 'surge_targets_met_pct' column after grouped by 'cluster'\n",
    "# bins = [-float('inf'), -8.5, -5.64, 0, 0.25, 0.35, 0.4, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0, 5.0, 5.64, float('inf')]\n",
    "# labels = ['< -8.5', '-8.5 to -5.64', '-5.64 to 0', '0 to 0.25', '0.25 to 0.35', '0.35 to 0.4', '0.4 to 0.5', '0.5 to 0.75', '0.75 to 1', '1 to 2', '2 to 3', '3 to 4', '4 to 5', '5 to 5.64', '> 5.64']\n",
    "# lc['binned'] = pd.cut(lc['surge_targets_met_pct'], bins=bins, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380098e5-f13d-4bed-ba94-0d4392fed1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "scoring = [] #algo, cluster quality metrics, cluster sizes, from topn\n",
    "for index, row in topn.iterrows():\n",
    "    data = m2_pipeline #need a temporary version of the master data\n",
    "\n",
    "    algo = row['algo']   # Extract the algorithm and cluster from the current row\n",
    "    c = row['clusters']\n",
    "    score = row['score']\n",
    "    kmeans = KMeans(n_clusters=c) #establish a means to build the cluster algo, cluster or no NEED SEEDING \n",
    "    hierarchical = AgglomerativeClustering(n_clusters=c) \n",
    "    spectral = SpectralClustering(n_clusters=c) \n",
    "    birch = Birch(n_clusters=c)\n",
    "    meanshift = MeanShift()\n",
    "    affinity = AffinityPropagation()\n",
    "    dbscan = DBSCAN()\n",
    "    optics = OPTICS()\n",
    "    algorithms = [(kmeans, 'KMeans'), (hierarchical, 'Agglomerative'), (spectral, 'Spectral'), (birch, 'Birch')\\\n",
    "                  ,(meanshift, 'Meanshift'), (affinity, 'Affinity'), (dbscan, 'DBSCAN'), (optics, 'OPTICS')]\n",
    "   \n",
    "    method = [t for t in algorithms if algo in t][0][0] #read the algo type from the topn list/df\n",
    "    print(\"handle: \",algo, c, score, method) #verify it is reading it ok\n",
    "    # method.fit(m2_pipeline)\n",
    "    predict = method.fit_predict(data) #fit predict, derive cluster labeling\n",
    "    data['cluster'] = pd.Series(predict, index=data.index) # Add a new column to the filtered dataframe with the predicted cluster labels\n",
    "    # group by cluster identifier, define ranges of values, number of clusters and their ranged values\n",
    "    #NEED A NEW ALGORITHM TO EVALUATE EACH CLUSTER, BY BUSINESS RULE\n",
    "    # means = data.groupby('Cluster')['surge_targets_met_pct'].mean() ##group by then aggregate\n",
    "    min_max_count = data.groupby('cluster')['surge_targets_met_pct'].agg(['min', 'max', 'count'])\n",
    "    print(type(min_max_count))\n",
    "#cluster analytics: \n",
    "    \n",
    "    fin = {\"algo\":method, \"score\":score, \"clusters\":c, \"metrics\":min_max_count}\n",
    "    print(fin)\n",
    "    scoring.append(fin)\n",
    "\n",
    "analytic = pd.DataFrame(scoring)\n",
    "print(analytic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7359847b-77b2-4b04-a338-cd54f4baf9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "analytic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5bb340-bd36-4621-a2d8-2dc47dd03cca",
   "metadata": {},
   "source": [
    "### take top performing cluster techniques, then ...\n",
    "\n",
    "attache labels to each and analyze the clustering profiles for efficiency, profit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec91dc-e6a0-4194-a7fe-e7c04cd31b42",
   "metadata": {},
   "source": [
    "## notes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed17c724-9a6c-4280-a565-b4a9f9904db8",
   "metadata": {},
   "source": [
    "1. grid search notes on [sklearn](https://scikit-learn.org/stable/modules/grid_search.html#grid-search)\n",
    "2. sklearn pipeline [guide](https://scikit-learn.org/stable/modules/compose.html#pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
