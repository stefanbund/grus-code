{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7c5d04-6844-47bc-b7ab-15572b05d1d2",
   "metadata": {},
   "source": [
    "## Cluster Technique Search and Accuracy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6873776c-9e64-4e97-83ba-a1c93e578d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "\n",
    "#SPECIFIED CLUSTER DISCOVERY\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering, Birch\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering,\\\n",
    "MeanShift, AffinityPropagation, DBSCAN, OPTICS, Birch\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "440ccac3-a41f-4b7d-acba-1c429cc35a00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "precursor_buy_cap_pct_change    float64\n",
       "precursor_ask_cap_pct_change    float64\n",
       "precursor_bid_vol_pct_change    float64\n",
       "precursor_ask_vol_pct_change    float64\n",
       "change.1                        float64\n",
       "surge_targets_met_pct           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BIN TECHNIQUE\n",
    "# load pipeline 1 csv and prep for clustering\n",
    "m2_pipeline = pd.read_csv('pipeline1.csv')\n",
    "# change is surge price rate of change per observation, change.1 is precursor\n",
    "# sum_change is surge sum_change per surge, and surge_area is surge alone\n",
    "keepable = ['precursor_buy_cap_pct_change', \n",
    "            'precursor_ask_cap_pct_change',\n",
    "            'precursor_bid_vol_pct_change', \n",
    "            'precursor_ask_vol_pct_change', 'change.1',\n",
    "            'surge_targets_met_pct']\n",
    "\n",
    "# Normalize the 'surge_targets_met_pct' column\n",
    "x = m2_pipeline[['surge_targets_met_pct']].values.astype(float)\n",
    "m2_pipeline = m2_pipeline[keepable]\n",
    "print(m2_pipeline.isna().sum(axis=1).astype(bool).sum())\n",
    "m2_pipeline = m2_pipeline.astype('float')\n",
    "m2_pipeline.dtypes\n",
    "\n",
    "# # bins = [float(f\"{x:.2f}\") for x in range(-10, 11)]\n",
    "# bins = [x * 0.2 for x in range(-10, 11)]\n",
    "# print(bins)\n",
    "# #model \n",
    "# m2_pipeline['bin'] = pd.cut(m2_pipeline['surge_targets_met_pct'], bins=bins  )#, labels=labels)\n",
    "# # Display the binned data\n",
    "# print(m2_pipeline['bin'].value_counts())\n",
    "# m2_pipeline['bin']\n",
    "# m2_pipeline['bin'] = m2_pipeline['bin'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85aeb500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-4.24232081911261,\n",
       " -2.6666666666666665,\n",
       " -1.3333333333333333,\n",
       " 0,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 1,\n",
       " 2,\n",
       " 5.322377307519139]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins = [\n",
    "    m2_pipeline['surge_targets_met_pct'].min() -1,  # Min value\n",
    "    -4/3*2,  # -4 to 0 divided into three equal parts\n",
    "    -4/3,  # Second bin edge for negative values\n",
    "    0,  # Zero separating negative and positive values\n",
    "    0.25, 0.5, 0.75, 1,  # Four bins between 0 and 1\n",
    "    2,  # One bin between 1 and 2\n",
    "    m2_pipeline['surge_targets_met_pct'].max() + 1]\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f8d6dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_pipeline['label'] = pd.cut(m2_pipeline['surge_targets_met_pct'], bins=bins, labels=list(range(1, len(bins))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "024682c2-746e-4162-9453-13016958db4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['precursor_buy_cap_pct_change', 'precursor_ask_cap_pct_change',\n",
       "       'precursor_bid_vol_pct_change', 'precursor_ask_vol_pct_change',\n",
       "       'change.1', 'surge_targets_met_pct', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2_pipeline.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5405cb9f-cfa2-44ee-bcf8-845a5bfd9a05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m2_pipeline.to_csv('binned_pipeline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ad79207-adab-4e2e-a5e9-473111635457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_test_results = [] #capture each test summary here, make df on this later\n",
    "profitability_analytics =[] #capture the value of each cluster, bin x price avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5efd8bd4-7b2d-4ea3-b849-b4f5e00df1fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cluster_profit(cluster):\n",
    "    print(cluster) #method, cluster count, silhouette score\n",
    "    # for the dataframe m2_pipeline, group by 'cluster' then 'bin', multiply bin as number by the average price by bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5f4762-7027-4dca-879a-ab1bf88dc9a5",
   "metadata": {},
   "source": [
    "### standardize all features pre train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10c9affa-6ded-45cc-89c5-d9949ae4e904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans:\n",
      "Best parameters: {'n_clusters': 2}\n",
      "Silhouette score (train): 0.8827\n",
      "Silhouette score (test): 0.8827\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# # Add the cluster label as a new column\u001b[39;00m\n\u001b[1;32m     44\u001b[0m method_df \u001b[38;5;241m=\u001b[39m X_train_scaled \u001b[38;5;66;03m#as it stands, then append a new column to it, write to csv\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m method_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[1;32m     47\u001b[0m now \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent date and time: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "\n",
    "m2_pipeline = m2_pipeline.dropna()\n",
    "\n",
    "# Splitting the dataframe into features and labels\n",
    "X = m2_pipeline.drop(columns=['label'])\n",
    "y = m2_pipeline['label']\n",
    "\n",
    "# Performing the test/train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "#DO STANDARDIZATION ONCE YOU DO TEST TRAIN NOT BEFORE \n",
    "def silhouette_scorer(estimator, X, y=None):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    return score\n",
    "# Defining the parameter grid for GridSearchCV\n",
    "param_grid = {'n_clusters': [2,3,4,5,6,7,8,9,10]}  #'algorithm': ['auto', 'full', 'elkan']\n",
    "\n",
    "clustering_models = [\n",
    "    ('KMeans', KMeans()),\n",
    "    # ('SpectralClustering', SpectralClustering(eigen_solver=None, n_components=None, random_state=42, n_init=10, gamma=1.0, affinity='rbf',\\\n",
    "    #                   n_neighbors=10, eigen_tol='auto', assign_labels='kmeans', degree=3, coef0=1, kernel_params=None, n_jobs=None, verbose=False)),\n",
    "    ('Birch', Birch()),\n",
    "    ('Hierarchical',AgglomerativeClustering())]\n",
    "# Performing GridSearchCV for each clustering model\n",
    "for model_name, model in clustering_models:\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring=silhouette_scorer)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    # Evaluating the best model based on silhouette score\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(f'{model_name}:')\n",
    "    print(f'Best parameters: {grid_search.best_params_}')\n",
    "    print(f'Silhouette score (train): {best_score:.4f}')\n",
    "    print(f'Silhouette score (test): {best_score:.4f}')\n",
    "    best_params = grid_search.best_params_\n",
    "    big_dict = grid_search.cv_results_\n",
    "    pkg = {\"algo\": model_name, \"best_params\": grid_search.best_params_,\n",
    "        \"best_estimator\": grid_search.best_estimator_,\n",
    "        \"best_score\": best_score, \"all_results\": big_dict} \n",
    "    # # Add the cluster label as a new column\n",
    "    method_df = X_train_scaled #as it stands, then append a new column to it, write to csv\n",
    "    method_df['cluster_label'] = best_model.labels_\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    print(\"Current date and time: \")\n",
    "    print(now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    run_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    file_name = 'clustered/'+ 'clustered_'+ model_name+' _'+ run_time+ '.csv'\n",
    "    print(file_name)\n",
    "    method_df.to_csv(file_name)\n",
    "    # print(pkg)\n",
    "    global_test_results.append(pkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06622dc5-9e4d-4430-8fbe-15cfc8986f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering, Birch\n",
    "from sklearn.cluster import MeanShift, AffinityPropagation, DBSCAN, OPTICS\n",
    "from sklearn.metrics import silhouette_score\n",
    "# Splitting the dataframe into features and labels\n",
    "X = m2_pipeline.drop(columns=['bin'])\n",
    "y = m2_pipeline['bin']\n",
    "# Performing the test/train split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "def silhouette_scorer(estimator, X, y=None):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    return score\n",
    "\n",
    "param_grid = {}  #'algorithm': ['auto', 'full', 'elkan']\n",
    "clustering_models = [\n",
    "    ('Meanshift',  MeanShift( bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300)),\n",
    "    ('AffinityPropagation', AffinityPropagation( damping=0.5, max_iter=200, convergence_iter=15, copy=True, preference=None, affinity='euclidean', verbose=False, random_state=42)),\n",
    "    ('DBSCAN', DBSCAN(eps=0.5,  min_samples=5, metric='euclidean', metric_params=None, algorithm='auto', leaf_size=30, p=None, n_jobs=None)),\n",
    "('OPTICS', OPTICS( min_samples=5, max_eps=3, metric='minkowski', p=2, metric_params=None, cluster_method='xi', eps=None, xi=0.05, predecessor_correction=True, min_cluster_size=None, algorithm='auto', leaf_size=30, memory=None, n_jobs=None))]\n",
    "# Performing GridSearchCV for each clustering model\n",
    "for model_name, model in clustering_models:\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring=silhouette_scorer)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    # Evaluating the best model based on silhouette score\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(f'{model_name}:')\n",
    "    # print(f'Best parameters: {grid_search.best_params_}')\n",
    "    print(f'Silhouette score (train): {best_score:.4f}')\n",
    "    print(f'Silhouette score (test): {best_score:.4f}')\n",
    "    big_dict = grid_search.cv_results_\n",
    "    pkg = {\"algo\":model_name, \"best_params\":grid_search.best_params_, \"best_estimator\":grid_search.best_estimator_,\\\n",
    "            \"best_score\":best_score, \"all_results\":big_dict}\n",
    "    # print(pkg)\n",
    "    global_test_results.append(pkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb955e5-3d93-4da6-8eba-95a76f5e451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import SpectralClustering\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_clusters': [2,3,4,5,6,7,8,9,10],\n",
    "#     'affinity': ['nearest_neighbors', 'rbf'],\n",
    "#     'gamma': [0.1, 1.0, 10.0]\n",
    "# }\n",
    "# def silhouette_scorer(estimator, X, y=None):\n",
    "#     labels = estimator.fit_predict(X)\n",
    "#     score = silhouette_score(X, labels)\n",
    "#     return score\n",
    "# # ('SpectralClustering', SpectralClustering(eigen_solver=None, n_components=None, random_state=42, n_init=10, gamma=1.0, affinity='rbf',\\\n",
    "#     #                   n_neighbors=10, eigen_tol='auto', assign_labels='kmeans', degree=3, coef0=1, kernel_params=None, n_jobs=None, verbose=False)),\n",
    "# clustering = SpectralClustering(eigen_solver=None, n_components=None, random_state=42, n_init=10, gamma=1.0, affinity='rbf',\\\n",
    "#                       n_neighbors=10, eigen_tol='auto', assign_labels='kmeans', degree=3, coef0=1, kernel_params=None, n_jobs=None, verbose=False)\n",
    "# grid_search = GridSearchCV(clustering, param_grid, scoring=silhouette_scorer)\n",
    "# grid_search.fit(X_train_scaled, y_train)\n",
    "# # Evaluating the best model based on silhouette score\n",
    "# best_model = grid_search.best_estimator_\n",
    "# best_score = grid_search.best_score_\n",
    "# print(f'{model_name}:')\n",
    "# # print(f'Best parameters: {grid_search.best_params_}')\n",
    "# print(f'Silhouette score (train): {best_score:.4f}')\n",
    "# print(f'Silhouette score (test): {best_score:.4f}')\n",
    "# big_dict = grid_search.cv_results_\n",
    "# pkg = {\"algo\":model_name, \"best_params\":grid_search.best_params_, \"best_estimator\":grid_search.best_estimator_,\\\n",
    "#         \"best_score\":best_score, \"all_results\":big_dict}\n",
    "# # print(pkg)\n",
    "# global_test_results.append(pkg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5d5dec-84e5-426f-a7e8-c78d01d952dc",
   "metadata": {},
   "source": [
    "## explore clustering methods by profitability\n",
    "\n",
    "apply the cluster id to the dataframe, group by, then summarize value by bin x price range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09f98f9-92de-453e-bf8d-72961c698a2d",
   "metadata": {},
   "source": [
    "## explore top results by cluster quality scoring\n",
    "\n",
    "use silhouette viz\n",
    "\n",
    "then davies\n",
    "\n",
    "then kolmogorov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b625c5d-576e-4e10-a2db-7c4e2c9ff563",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(global_test_results)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd096ad6-dba8-4226-be34-4002f520c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#charting silhouettes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "# Calculate the silhouette scores for each sample\n",
    "silhouette_values = silhouette_samples(X, labels)\n",
    "\n",
    "# Calculate the average silhouette score\n",
    "average_score = silhouette_score(X, labels)\n",
    "\n",
    "# Plot the silhouette chart\n",
    "fig, ax = plt.subplots()\n",
    "y_lower = 10\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    # Aggregate the silhouette scores for samples in cluster i and sort them\n",
    "    ith_cluster_silhouette_values = silhouette_values[labels == i]\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    # Fill the silhouette chart with the corresponding color\n",
    "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "    # Label each cluster with its silhouette score\n",
    "    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "    \n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "ax.set_xlabel(\"Silhouette coefficient values\")\n",
    "ax.set_ylabel(\"Cluster label\")\n",
    "\n",
    "# The vertical line indicates the average silhouette score\n",
    "ax.axvline(x=average_score, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c7312-02c2-4b6b-af2f-9f044b677acb",
   "metadata": {},
   "source": [
    "## top method hyperparameter optimization\n",
    "take the top technique by silhouette then dive into a fuller exploration of its specific hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987147d-516e-4e6d-8990-59f2d27a2379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    'n_clusters': [2, 3, 4],\n",
    "    'affinity': ['nearest_neighbors', 'rbf'],\n",
    "    'gamma': [0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "clustering = SpectralClustering()\n",
    "random_search = RandomizedSearchCV(clustering, param_distributions)\n",
    "random_search.fit(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b99ad-37c5-4d5f-8007-5bffa28473c7",
   "metadata": {},
   "source": [
    "## estimate the model accuracy, givne best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8d6b0-fe5b-401a-ba7f-2f54e479f8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#use best model:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import accuracy_score\n",
    "X = m2_pipeline.drop(columns=['bin'])\n",
    "y = m2_pipeline['bin']\n",
    "# Splitting the dataset into train and test sets\n",
    "\n",
    "#USE STANDARD SCALING ONCE YOU DIVDE TEST AND TRAIN NOT BEFORE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating an instance of SpectralClustering\n",
    "model = SpectralClustering(n_clusters=7)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "model.fit_predict(X_train)\n",
    "\n",
    "# Predicting labels for the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc582b",
   "metadata": {},
   "source": [
    "## contrast silhouette and scatter in one grid\n",
    "\n",
    "[source](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#selecting-the-number-of-clusters-with-silhouette-analysis-on-kmeans-clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ab9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "# Generating the sample data from make_blobs\n",
    "# This particular setting has one distinct cluster and 3 clusters placed close\n",
    "# together.\n",
    "X, y = make_blobs(\n",
    "    n_samples=500,\n",
    "    n_features=2,\n",
    "    centers=4,\n",
    "    cluster_std=1,\n",
    "    center_box=(-10.0, 10.0),\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    ")  # For reproducibility\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, n_init=\"auto\", random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(\n",
    "        centers[:, 0],\n",
    "        centers[:, 1],\n",
    "        marker=\"o\",\n",
    "        c=\"white\",\n",
    "        alpha=1,\n",
    "        s=200,\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
