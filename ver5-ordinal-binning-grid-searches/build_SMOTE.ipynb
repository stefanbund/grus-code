{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Requirement already satisfied: imbalanced-learn in /home/stefan/.local/lib/python3.8/site-packages (from imblearn) (0.11.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/stefan/.local/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/stefan/.local/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /home/stefan/.local/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/stefan/.local/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/stefan/.local/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (3.2.0)\n",
      "Installing collected packages: imblearn\n",
      "Successfully installed imblearn-0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## expand a labeled dataset to SMOTE\n",
    "\n",
    "magnify the number of minority/exceptional cases within the sequence dataset, ideally targets the binary binned dataset.\n",
    "\n",
    "[reference 1](<ver5-ordinal-binning-grid-searches/step 2-0, ranged clustering, with time.ipynb>)\n",
    "\n",
    "different oversampling tools: Naive random oversampling, SMOTE, ADASYN, SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE,SMOTENC, KMeansSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from collections import Counter\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from imblearn.ensemble import BalancedBaggingClassifier,BalancedRandomForestClassifier,RUSBoostClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, GradientBoostingClassifier, RandomForestClassifier, \\\n",
    "ExtraTreesClassifier, RandomTreesEmbedding, BaggingClassifier\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import shap\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import altair as alt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_pipeline = pd.read_csv(\"binary_binned_pipeline.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'precursor_buy_cap_pct_change',\n",
       "       'precursor_ask_cap_pct_change', 'precursor_bid_vol_pct_change',\n",
       "       'precursor_ask_vol_pct_change', 'sum_change', 'length',\n",
       "       'surge_targets_met_pct', 'time', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2_pipeline.columns #do this to identify the index of the categorical feature, for below setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up toolsets as functions to build separate datasets, bin_Naive, bin_SMOTE, bin_ADASYN\n",
    "\n",
    "def build_naive(d):  #https://imbalanced-learn.org/stable/over_sampling.html#naive-random-over-sampling\n",
    "    ros = RandomOverSampler(random_state=42, sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    print(\"ROS\",sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def build_smote(d): #https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html#smote\n",
    "    X_resampled, y_resampled = SMOTE(random_state=42,categorical_features='label', categorical_encoder=None, ).fit_resample(X, y)\n",
    "    print(\"SMOTE\",sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def build_adasyn(d): #https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.ADASYN.html#adasyn\n",
    "    X_resampled, y_resampled = ADASYN(random_state=42,sampling_strategy='minority').fit_resample(X, y)\n",
    "    print(\"ADASYN\",sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def build_borderline(d): #https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.BorderlineSMOTE.html#borderlinesmote\n",
    "    X_resampled, y_resampled = BorderlineSMOTE(random_state=42,sampling_strategy='minority').fit_resample(X, y)\n",
    "    print(\"BORDERLINE\",sorted(Counter(y_resampled).items())) \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def build_smotenc(d): #https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.html#smotenc\n",
    "    smote_nc = SMOTENC(categorical_features=\"infer\", random_state=42,sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = smote_nc.fit_resample(X, y)\n",
    "    print(\"SMOTENC\",sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def build_svmsmote(d):\n",
    "    sm = SVMSMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "    print(\"SVMSMOTE\",'Resampled dataset shape %s' % Counter(y_res))\n",
    "    return X_res, y_res\n",
    "\n",
    "def build_kmsmote(d): #https://imbalanced-learn.org/stable/combine.html#combination-of-over-and-under-sampling\n",
    "    m = KMeansSMOTE(kmeans_estimator=MiniBatchKMeans(n_init=1, random_state=42), random_state=42,sampling_strategy='minority')\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "    # Find the number of new samples in the middle blob\n",
    "    n_res_in_middle = ((X_res[:, 0] > -5) & (X_res[:, 0] < 5)).sum()\n",
    "    print(\"KMSMOTE\",\"Samples in the middle blob: %s\" % n_res_in_middle)\n",
    "    return X_res, y_res\n",
    "\n",
    "def build_smoteenn(d):\n",
    "    smote_enn = SMOTEENN(random_state=42,sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "    print(\"SMOTEENN\",sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    " def build_smotetomek(d):\n",
    "    smote_tomek = SMOTETomek(random_state=42,sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = smote_tomek.fit_resample(X, y)\n",
    "    print(\"SMOTETOMEK\",sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take optimal classifier parameters and technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the dataframe into features and labels\n",
    "# X = m2_pipeline.drop(columns=['label'])\n",
    "def getBestClassifier(name, dataset):   #'kmsmote', build_svmsmote(source)\n",
    "    # y = m2_pipeline['label'].values #per https://stackoverflow.com/a/73095562/12001832\n",
    "    # X = m2_pipeline[keepable].values\n",
    "    y = dataset[1].values\n",
    "    X = dataset[0].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()  #normalize all numeric columns\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    classifiers = [  # Define the classifiers and their respective hyperparameters\n",
    "        RandomForestClassifier(),\n",
    "        GradientBoostingClassifier(),\n",
    "        HistGradientBoostingClassifier(),\n",
    "        ExtraTreesClassifier(),\n",
    "        BaggingClassifier(),\n",
    "        RidgeCV(),\n",
    "        LassoCV(),\n",
    "        SVC(),\n",
    "        LogisticRegression(),\n",
    "        BernoulliNB(),\n",
    "        KNNeighbors(),\n",
    "    ]\n",
    "    params = {\n",
    "        'RandomForestClassifier': {'n_estimators': [10, 100, 1000], 'max_depth': [None, 10, 100]},\n",
    "        'GradientBoostingClassifier': {'n_estimators': [10, 100, 1000], 'max_depth': [None, 10, 100]},\n",
    "        'HistGradientBoostingClassifier': {'learning_rate': [0.1, 0.01], 'max_iter': [100, 200]},\n",
    "        'ExtraTreesClassifier':{'n_estimators': [10, 100, 1000], 'max_depth': [None, 10, 100]},\n",
    "        'BaggingClassifier':{ 'n_estimators':[10],  'random_state':[42]},\n",
    "        'RidgeCV':{'alphas':[0.1, 1.0, 10.0]},\n",
    "        'LassoCV':{ 'eps':[0.001, 0.01, .1], 'n_alphas':[100,200],  'max_iter':[100,200,300,1000]},\n",
    "        'SVC':{'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "        'LogisticRegression':{'C': [0.1, 1, 10], 'penalty':['l1','l2','elasticnet','None'], 'multi_class':['ovr','auto'],\\\n",
    "                              'random_state':[42]},\n",
    "        'BernoulliNB':{'fit_prior':[True, False]},\n",
    "        'KNNeighbors':{'n_neighbors':[3,4,5,6,7,8], 'algorithm':['auto'], 'n_jobs':[1,2,3,4]}\n",
    "    }\n",
    "    comparative = []\n",
    "    # Perform the grid search\n",
    "    for clf in classifiers:\n",
    "        name = clf.__class__.__name__\n",
    "        if name in params:\n",
    "            grid_search = GridSearchCV(clf, params[name], cv=5)\n",
    "            grid_search.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "            accuracy = grid_search.score(X_test, y_test)\n",
    "            \n",
    "            dict = {\"classifier\":name, \"best_params\":grid_search.best_params_, \"accuracy\":accuracy}\n",
    "            comparative.append(dict)\n",
    "    return(comparative)\n",
    "    # dg = pd.DataFrame(comparative) #display grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trigger the search mechanism, per SMOTE method\n",
    "\n",
    "#loop through each oversampled dataset, then run the classifier search on that set, outlining each set, before hand\n",
    "\n",
    "#for each set, run the classifier search\n",
    "source = 'binary_binned_pipeline.csv'\n",
    "resultSet = []\n",
    "resultSet.concat(getBestClassifier('naive', build_naive(source)))  #many rows\n",
    "resultSet.concat(getBestClassifier('smote', build_smote(source)))\n",
    "resultSet.concat(getBestClassifier('adasyn', build_adasyn(source)))\n",
    "resultSet.concat(getBestClassifier('borderline', build_borderline(source)))\n",
    "resultSet.concat(getBestClassifier('smotenc', build_smotenc(source)))\n",
    "resultSet.concat(getBestClassifier('svmsmote', build_svmsmote(source)))\n",
    "resultSet.concat(getBestClassifier('kmsmote', build_kmsmote(source)))\n",
    "resultSet.concat(getBestClassifier('smoteenn', build_smoteenn(source)))\n",
    "resultSet.concat(getBestClassifier('smotetomek', build_smotetomek(source)))\n",
    "\n",
    "optimals = pd.DataFrame(resultSet)\n",
    "optimals.groupby(column=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pitfalls of oversampling](https://imbalanced-learn.org/stable/common_pitfalls.html#data-leakage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sampling based ensemble methods\n",
    "\n",
    "score each, can you combine into a voter?\n",
    "\n",
    "[validation curve model selection](https://imbalanced-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html#plotting-validation-curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced bagger\n",
    "\n",
    "bbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                sampling_strategy='not majority',\n",
    "                                replacement=False,\n",
    "                                random_state=42)\n",
    "bbc.fit(X_train, y_train)\n",
    "y_pred = bbc.predict(X_test)\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balanced tree estimator\n",
    "brf = BalancedRandomForestClassifier(\n",
    "    n_estimators=100, random_state=42, sampling_strategy=\"all\", replacement=True\n",
    ")\n",
    "brf.fit(X_train, y_train)\n",
    "y_pred = brf.predict(X_test)\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rusboost = RUSBoostClassifier(n_estimators=200, algorithm='SAMME.R',\n",
    "                              random_state=42)\n",
    "rusboost.fit(X_train, y_train)\n",
    "y_pred = rusboost.predict(X_test)\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap #this library would take 30 hours to explain the above model, not to be used. would function on one tree, better\n",
    "from sklearn.ensemble import  VotingClassifier \n",
    "from sklearn.model_selection import cross_val_score\n",
    "X = m2_pipeline[keepable].values  #.drop(columns=['label']).values #per https://stackoverflow.com/a/73095562/12001832\n",
    "y = m2_pipeline['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#normalize all numeric columns\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Define VotingClassifier \n",
    "weights = [0.1, 0.5, 0.1, 0.1, 0.2]\n",
    "vc = VotingClassifier(estimators=[('rfc',rfc),('gbc',gbc),('hgbc',hgbc),('etc',etc),('bc',bc)], voting='hard', weights=weights) \n",
    "rfc.fit(X_train_scaled,y_train)\n",
    "gbc.fit(X_train_scaled,y_train)\n",
    "hgbc.fit(X_train_scaled,y_train)\n",
    "etc.fit(X_train_scaled,y_train)\n",
    "bc.fit(X_train_scaled,y_train)\n",
    "svc.fit(X_train_scaled,y_train)\n",
    "vc.fit(X_train_scaled,y_train)\n",
    "\n",
    "# #fit all, voting classifier scoring\n",
    "for clf, label in zip([rfc,gbc,hgbc,etc,bc,svc,vc], ['RandomForestClassifier', 'GradientBoostingClassifier', 'HistGradientBoostingClassifier', 'ExtraTreesClassifier',\\\n",
    "                                                    'BaggingClassifier','SVC','Voting']):\n",
    "    scores = cross_val_score(clf, X_train_scaled, y_train, scoring='accuracy', cv=5)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
