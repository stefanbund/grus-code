{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains the csv output for clustering resampled binary label data\n",
    "# Still needs cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand a labeled dataset to SMOTE then Cluster\n",
    "\n",
    "magnify the number of minority/exceptional cases within the sequence dataset, ideally targets the binary binned dataset.\n",
    "\n",
    "[reference 1](<ver5-ordinal-binning-grid-searches/step 2-0, ranged clustering, with time.ipynb>)\n",
    "\n",
    "different oversampling tools: Naive random oversampling, SMOTE, ADASYN, SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE,SMOTENC, KMeansSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from collections import Counter\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from imblearn.ensemble import BalancedBaggingClassifier,BalancedRandomForestClassifier,RUSBoostClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, GradientBoostingClassifier, RandomForestClassifier, \\\n",
    "ExtraTreesClassifier, RandomTreesEmbedding, BaggingClassifier\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#import shap\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import altair as alt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import  VotingClassifier \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_pipeline = pd.read_csv(\"binary_binned_pipeline.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'precursor_buy_cap_pct_change',\n",
       "       'precursor_ask_cap_pct_change', 'precursor_bid_vol_pct_change',\n",
       "       'precursor_ask_vol_pct_change', 'sum_change', 'length',\n",
       "       'surge_targets_met_pct', 'time', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2_pipeline.columns #do this to identify the index of the categorical feature, for below setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keepable = ['precursor_buy_cap_pct_change', \n",
    "            'precursor_ask_cap_pct_change',\n",
    "            'precursor_bid_vol_pct_change', \n",
    "            'precursor_ask_vol_pct_change',\n",
    "            'sum_change','length','time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = m2_pipeline['label'] #per https://stackoverflow.com/a/73095562/12001832\n",
    "X = m2_pipeline[keepable]\n",
    "# Performing the test/train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#normalize all numeric columns\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up toolsets as functions to build separate datasets, bin_Naive, bin_SMOTE, bin_ADASYN\n",
    "\n",
    "def build_naive():  #https://imbalanced-learn.org/stable/over_sampling.html#naive-random-over-sampling\n",
    "    ros = RandomOverSampler(random_state=42, sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    #print(\"ROS\",sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def build_smote(): #https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html#smote\n",
    "    X_resampled, y_resampled = SMOTE(random_state=42 ).fit_resample(X, y)\n",
    "    #print(\"SMOTE\",sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def build_adasyn(): #https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.ADASYN.html#adasyn\n",
    "    X_resampled, y_resampled = ADASYN(random_state=42,sampling_strategy='minority').fit_resample(X, y)\n",
    "    #print(\"ADASYN\",sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def build_borderline(): #https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.BorderlineSMOTE.html#borderlinesmote\n",
    "    X_resampled, y_resampled = BorderlineSMOTE(random_state=42,sampling_strategy='minority').fit_resample(X, y)\n",
    "    #print(\"BORDERLINE\",sorted(Counter(y_resampled).items())) \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def build_smotenc(): #https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.html#smotenc\n",
    "    smote_nc = SMOTENC( random_state=42,sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = smote_nc.fit_resample(X, y)\n",
    "    #print(\"SMOTENC\",sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def build_svmsmote():\n",
    "    sm = SVMSMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "    #print(\"SVMSMOTE\",'Resampled dataset shape %s' % Counter(y_res))\n",
    "    return X_res, y_res\n",
    "\n",
    "def build_kmsmote(): #https://imbalanced-learn.org/stable/combine.html#combination-of-over-and-under-sampling\n",
    "    m = KMeansSMOTE( random_state=42,sampling_strategy='minority')\n",
    "    X_res, y_res = m.fit_resample(X, y)\n",
    "    # Find the number of new samples in the middle blob\n",
    "    n_res_in_middle = ((X_res[:, 0] > -5) & (X_res[:, 0] < 5)).sum()\n",
    "    #print(\"KMSMOTE\",\"Samples in the middle blob: %s\" % n_res_in_middle)\n",
    "    return X_res, y_res\n",
    "\n",
    "def build_smoteenn():\n",
    "    smote_enn = SMOTEENN(random_state=42,sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "    #print(\"SMOTEENN\",sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def build_smotetomek():\n",
    "    smote_tomek = SMOTETomek(random_state=42,sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = smote_tomek.fit_resample(X, y)\n",
    "    #print(\"SMOTETOMEK\",sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take optimal classifier parameters and technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_clusters = 2\n",
    "def silhouette_scorer(estimator, X, y=None):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplers = [build_naive,build_smote,build_adasyn,build_borderline,build_svmsmote,build_smoteenn,build_smotetomek]\n",
    "samplers_string = [\"build_naive\",\"build_smote\",\"build_adasyn\", \"build_borderline\", \"build_svmsmote\", \"build_smoteenn\",\"build_smotetomek\"]\n",
    "sampler_dict = {k: v for k, v in zip(samplers_string, samplers )}\n",
    "\n",
    "n_clusters = [2,4,6,8,10,12]\n",
    "data_list = []\n",
    "for clus in n_clusters:\n",
    "    for key, value in sampler_dict.items():\n",
    "        X_sampled, y_sampled = value()\n",
    "        #normalize all numeric columns\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_sampled)\n",
    "        clf = KMeans(n_clusters =clus, init='k-means++', random_state = 42)\n",
    "        labels = clf.fit_predict(X_train_scaled)\n",
    "\n",
    "        df = pd.DataFrame({'y_sampled': y_sampled, 'Cluster_Label': labels})\n",
    "\n",
    "        # Calculate the ratio of the larger count of labels (0 or 1) to the total count in each cluster\n",
    "        cluster_ratios = df.groupby('Cluster_Label')['y_sampled'].value_counts().unstack().fillna(0)\n",
    "        cluster_max_ratio = cluster_ratios.max(axis=1)\n",
    "        cluster_total_count = cluster_ratios.sum(axis=1)\n",
    "        cluster_profit_scores = cluster_max_ratio / cluster_total_count\n",
    "\n",
    "        # Calculate the average profit score across clusters\n",
    "        average_profit_score = np.mean(cluster_profit_scores)\n",
    "        silhouette_score_1 = silhouette_score(X_train_scaled, labels)\n",
    "        data_list.append({\n",
    "            'clusters': clus,\n",
    "            'model': key,\n",
    "            'silhouette_score': silhouette_score_1,\n",
    "            'profit_score': average_profit_score\n",
    "        })\n",
    "sampled_df = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clusters                      2\n",
       "model               build_smote\n",
       "silhouette_score       0.793449\n",
       "profit_score           0.715419\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df.loc[sampled_df['silhouette_score'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clusters                      12\n",
       "model               build_adasyn\n",
       "silhouette_score        0.263896\n",
       "profit_score            0.762506\n",
       "Name: 37, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df.loc[sampled_df['profit_score'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sampled, y_sampled = build_smote()\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_sampled)\n",
    "clf = KMeans(n_clusters =2, init='k-means++', random_state = 42)\n",
    "labels = clf.fit_predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final output csv containing most successful resampled data with cluster label \n",
    "df = pd.concat([X_sampled.reset_index(drop=True), pd.Series(labels, name='Cluster_Label')], axis=1)\n",
    "df = pd.concat([df, y_sampled], axis =1)\n",
    "df.to_csv(\"binary_clustered_resampled_pipeline.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
