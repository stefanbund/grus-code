{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c30f8bf-652d-44fa-9a25-e7288788ef53",
   "metadata": {},
   "source": [
    "## classifier method search\n",
    "\n",
    "premise: use a looped grid search to explore the accuracy metrics of classfiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3068694-fdd8-46ab-a2cf-3fbc8aaf6bef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, GradientBoostingClassifier, RandomForestClassifier, \\\n",
    "ExtraTreesClassifier, RandomTreesEmbedding, BaggingClassifier\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "953e943e-f4b3-456f-8019-8d979d44c22c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "m2_pipeline = pd.read_csv('binned_pipeline.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b1b8c-62cb-4b02-8586-b19a9798f709",
   "metadata": {},
   "source": [
    "## testing data for colinearity among features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "339e4a9d-32be-4304-adb6-7c4be7ab83e3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>precursor_buy_cap_pct_change</th>\n",
       "      <th>precursor_ask_cap_pct_change</th>\n",
       "      <th>precursor_bid_vol_pct_change</th>\n",
       "      <th>precursor_ask_vol_pct_change</th>\n",
       "      <th>change.1</th>\n",
       "      <th>surge_targets_met_pct</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.074820</td>\n",
       "      <td>-0.036954</td>\n",
       "      <td>-0.077781</td>\n",
       "      <td>0.017377</td>\n",
       "      <td>-0.037113</td>\n",
       "      <td>-0.002245</td>\n",
       "      <td>-0.040018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precursor_buy_cap_pct_change</th>\n",
       "      <td>-0.074820</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.218251</td>\n",
       "      <td>0.825875</td>\n",
       "      <td>0.082502</td>\n",
       "      <td>-0.015773</td>\n",
       "      <td>0.007553</td>\n",
       "      <td>0.000636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precursor_ask_cap_pct_change</th>\n",
       "      <td>-0.036954</td>\n",
       "      <td>0.218251</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.340009</td>\n",
       "      <td>0.092719</td>\n",
       "      <td>-0.005007</td>\n",
       "      <td>-0.020821</td>\n",
       "      <td>-0.016212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precursor_bid_vol_pct_change</th>\n",
       "      <td>-0.077781</td>\n",
       "      <td>0.825875</td>\n",
       "      <td>0.340009</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011741</td>\n",
       "      <td>-0.017284</td>\n",
       "      <td>0.005475</td>\n",
       "      <td>-0.001806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precursor_ask_vol_pct_change</th>\n",
       "      <td>0.017377</td>\n",
       "      <td>0.082502</td>\n",
       "      <td>0.092719</td>\n",
       "      <td>0.011741</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018066</td>\n",
       "      <td>0.020034</td>\n",
       "      <td>0.024472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>change.1</th>\n",
       "      <td>-0.037113</td>\n",
       "      <td>-0.015773</td>\n",
       "      <td>-0.005007</td>\n",
       "      <td>-0.017284</td>\n",
       "      <td>0.018066</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.051610</td>\n",
       "      <td>0.079098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surge_targets_met_pct</th>\n",
       "      <td>-0.002245</td>\n",
       "      <td>0.007553</td>\n",
       "      <td>-0.020821</td>\n",
       "      <td>0.005475</td>\n",
       "      <td>0.020034</td>\n",
       "      <td>0.051610</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.817659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>-0.040018</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>-0.016212</td>\n",
       "      <td>-0.001806</td>\n",
       "      <td>0.024472</td>\n",
       "      <td>0.079098</td>\n",
       "      <td>0.817659</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Unnamed: 0  precursor_buy_cap_pct_change  \\\n",
       "Unnamed: 0                      1.000000                     -0.074820   \n",
       "precursor_buy_cap_pct_change   -0.074820                      1.000000   \n",
       "precursor_ask_cap_pct_change   -0.036954                      0.218251   \n",
       "precursor_bid_vol_pct_change   -0.077781                      0.825875   \n",
       "precursor_ask_vol_pct_change    0.017377                      0.082502   \n",
       "change.1                       -0.037113                     -0.015773   \n",
       "surge_targets_met_pct          -0.002245                      0.007553   \n",
       "label                          -0.040018                      0.000636   \n",
       "\n",
       "                              precursor_ask_cap_pct_change  \\\n",
       "Unnamed: 0                                       -0.036954   \n",
       "precursor_buy_cap_pct_change                      0.218251   \n",
       "precursor_ask_cap_pct_change                      1.000000   \n",
       "precursor_bid_vol_pct_change                      0.340009   \n",
       "precursor_ask_vol_pct_change                      0.092719   \n",
       "change.1                                         -0.005007   \n",
       "surge_targets_met_pct                            -0.020821   \n",
       "label                                            -0.016212   \n",
       "\n",
       "                              precursor_bid_vol_pct_change  \\\n",
       "Unnamed: 0                                       -0.077781   \n",
       "precursor_buy_cap_pct_change                      0.825875   \n",
       "precursor_ask_cap_pct_change                      0.340009   \n",
       "precursor_bid_vol_pct_change                      1.000000   \n",
       "precursor_ask_vol_pct_change                      0.011741   \n",
       "change.1                                         -0.017284   \n",
       "surge_targets_met_pct                             0.005475   \n",
       "label                                            -0.001806   \n",
       "\n",
       "                              precursor_ask_vol_pct_change  change.1  \\\n",
       "Unnamed: 0                                        0.017377 -0.037113   \n",
       "precursor_buy_cap_pct_change                      0.082502 -0.015773   \n",
       "precursor_ask_cap_pct_change                      0.092719 -0.005007   \n",
       "precursor_bid_vol_pct_change                      0.011741 -0.017284   \n",
       "precursor_ask_vol_pct_change                      1.000000  0.018066   \n",
       "change.1                                          0.018066  1.000000   \n",
       "surge_targets_met_pct                             0.020034  0.051610   \n",
       "label                                             0.024472  0.079098   \n",
       "\n",
       "                              surge_targets_met_pct     label  \n",
       "Unnamed: 0                                -0.002245 -0.040018  \n",
       "precursor_buy_cap_pct_change               0.007553  0.000636  \n",
       "precursor_ask_cap_pct_change              -0.020821 -0.016212  \n",
       "precursor_bid_vol_pct_change               0.005475 -0.001806  \n",
       "precursor_ask_vol_pct_change               0.020034  0.024472  \n",
       "change.1                                   0.051610  0.079098  \n",
       "surge_targets_met_pct                      1.000000  0.817659  \n",
       "label                                      0.817659  1.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2_pipeline.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c33e08-0303-46f3-8242-460e0cdcd80b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = m2_pipeline.corr(numeric_only=True)\n",
    "\n",
    "# Plot the correlation matrix\n",
    "sns.heatmap(corr_matrix, annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f52e2-cba3-4b25-a42d-052c87d7b933",
   "metadata": {},
   "source": [
    "XGB, GBM, CAT, FASTAI, NN_TORCH, LR, RF, XT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f675a98-ca95-40f2-857c-7b3dd898e714",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def deliver_models(): #deliver a matrix of classifier definitions\n",
    "    random_state =42\n",
    "    return [\n",
    "        ('HistGradientBoostingClassifier',  HistGradientBoostingClassifier(loss='log_loss', learning_rate=0.1, max_iter=100, \\\n",
    "                                                                               max_leaf_nodes=31, max_depth=None, min_samples_leaf=20, \\\n",
    "                                                                               l2_regularization=0.0, max_bins=255, categorical_features=False, \\\n",
    "                                                                               monotonic_cst=None, interaction_cst=None, warm_start=False, \\\n",
    "                                                                               early_stopping='auto', scoring='loss', validation_fraction=0.1, \\\n",
    "                                                                               n_iter_no_change=10, tol=1e-07, verbose=0, random_state=42, \\\n",
    "                                                                               class_weight=None )),\n",
    "    ('GradientBoostingClassifier', GradientBoostingClassifier(loss='log_loss', learning_rate=0.1, n_estimators=100, subsample=1.0, \\\n",
    "                                                       criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, \\\n",
    "                                                       min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, \\\n",
    "                                                       init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, \\\n",
    "                                                       warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)),\n",
    "    ('RandomForestClassifier', RandomForestClassifier(n_estimators=100,  criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1,\\\n",
    "                                                      min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, \\\n",
    "                                                      min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, \\\n",
    "                                                      random_state=42, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0,\\\n",
    "                                                      max_samples=None)),\n",
    "    ('ExtraTreesClassifier', ExtraTreesClassifier(n_estimators=100,  criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, \\\n",
    "                                                  min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0,\\\n",
    "                                                  bootstrap=False, oob_score=False, n_jobs=None, random_state=42, verbose=0, warm_start=False,\\\n",
    "                                                  class_weight=None, ccp_alpha=0.0, max_samples=None)),\n",
    "    ('RandomTreesEmbedding', RandomTreesEmbedding(n_estimators=100, max_depth=5, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \\\n",
    "                          max_leaf_nodes=None, min_impurity_decrease=0.0, sparse_output=True, n_jobs=None, random_state=42, verbose=0, \\\n",
    "                          warm_start=False)),\n",
    "    ('BaggingClassifier', BaggingClassifier(estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True,\\\n",
    "                                            bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=42, \\\n",
    "                                            verbose=0, base_estimator='deprecated')),\n",
    "    ('CatBoostEncoder', ce.CatBoostEncoder(verbose=0, cols='bin', drop_invariant=False, return_df=True, handle_unknown='value', handle_missing='value', \\\n",
    "                        random_state=42, sigma=None, a=1)),\n",
    "    ('RidgeCV',RidgeCV(alphas=(0.1, 1.0, 10.0), fit_intercept=True, scoring=None, cv=None, gcv_mode=None, store_cv_values=False, \\\n",
    "                       alpha_per_target=False)),\n",
    "    ('LassoCV',LassoCV( eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, precompute='auto', max_iter=1000, tol=0.0001, copy_X=True, cv=None, \\\n",
    "                verbose=False, n_jobs=None, positive=False, random_state=42, selection='cyclic')),\n",
    "    ('SVC', SVC( C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, \\\n",
    "                class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=42))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4844c174-6ae8-4ba9-9e34-4ddf61f4cc2f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                        int64\n",
       "precursor_buy_cap_pct_change    float64\n",
       "precursor_ask_cap_pct_change    float64\n",
       "precursor_bid_vol_pct_change    float64\n",
       "precursor_ask_vol_pct_change    float64\n",
       "change.1                        float64\n",
       "surge_targets_met_pct           float64\n",
       "label                             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2_pipeline.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d11e57b4-9c36-4940-acb6-6e47be81d829",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The least populated class in y has only 2 members, which is less than n_splits=5.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m     41\u001b[0m     grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(clf, params[name], cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mscore(X_test, y_test)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1389\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    824\u001b[0m         clone(base_estimator),\n\u001b[1;32m    825\u001b[0m         X,\n\u001b[1;32m    826\u001b[0m         y,\n\u001b[1;32m    827\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    828\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    829\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    830\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    831\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    833\u001b[0m     )\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    835\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    836\u001b[0m     )\n\u001b[1;32m    837\u001b[0m )\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:476\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    465\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    468\u001b[0m ]\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 476\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[1;32m    477\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    478\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    479\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    480\u001b[0m )(\n\u001b[1;32m    481\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    482\u001b[0m         t,\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[1;32m    484\u001b[0m         X,\n\u001b[1;32m    485\u001b[0m         y,\n\u001b[1;32m    486\u001b[0m         sample_weight,\n\u001b[1;32m    487\u001b[0m         i,\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[1;32m    489\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    490\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[1;32m    491\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[1;32m    492\u001b[0m     )\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[1;32m    494\u001b[0m )\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:189\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    187\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 189\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/tree/_classes.py:969\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    940\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \n\u001b[1;32m    942\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 969\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    970\u001b[0m         X,\n\u001b[1;32m    971\u001b[0m         y,\n\u001b[1;32m    972\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    973\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[1;32m    974\u001b[0m     )\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/tree/_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    449\u001b[0m         splitter,\n\u001b[1;32m    450\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    456\u001b[0m     )\n\u001b[0;32m--> 458\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight)\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Splitting the dataframe into features and labels\n",
    "X = m2_pipeline.drop(columns=['label'])\n",
    "y = m2_pipeline['label']\n",
    "\n",
    "# Performing the test/train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the classifiers and their respective hyperparameters\n",
    "classifiers = [\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    HistGradientBoostingClassifier(),\n",
    "    ExtraTreesClassifier(),\n",
    "    BaggingClassifier(),\n",
    "    # ce.CatBoostEncoder(),\n",
    "    # RandomTreesEmbedding(),\n",
    "    RidgeCV(),\n",
    "    LassoCV(),\n",
    "    # SVC()\n",
    "]\n",
    "\n",
    "params = {\n",
    "    'RandomForestClassifier': {'n_estimators': [10, 100, 1000], 'max_depth': [None, 10, 100]},\n",
    "    'GradientBoostingClassifier': {'n_estimators': [10, 100, 1000], 'max_depth': [None, 10, 100]},\n",
    "    'HistGradientBoostingClassifier': {'learning_rate': [0.1, 0.01], 'max_iter': [100, 200]},\n",
    "    'ExtraTreesClassifier':{'n_estimators': [10, 100, 1000], 'max_depth': [None, 10, 100]},\n",
    "    # 'RandomTreesEmbedding':{'n_estimators': [10, 100, 1000], 'max_depth': [None, 10, 100]},\n",
    "    'BaggingClassifier':{ 'n_estimators':[10],  'random_state':[42]},\n",
    "    # 'CatBoostEncoder':{'cols':['label'], 'drop_invariant':False, 'return_df':False, 'random_state':[42]},\n",
    "    'RidgeCV':{'alphas':[0.1, 1.0, 10.0]},\n",
    "    'LassoCV':{ 'eps':[0.001, 0.01, .1], 'n_alphas':[100,200],  'max_iter':[100,200,300,1000]},\n",
    "    # 'SVC':{'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "}\n",
    "comparative = []\n",
    "# Perform the grid search\n",
    "for clf in classifiers:\n",
    "    name = clf.__class__.__name__\n",
    "    if name in params:\n",
    "        grid_search = GridSearchCV(clf, params[name], cv=5)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "        accuracy = grid_search.score(X_test, y_test)\n",
    "        dict = {\"classifier\":name, \"best_params\":grid_search.best_params_, \"accuracy\":accuracy}\n",
    "        comparative.append(dict)\n",
    "dg = pd.DataFrame(comparative) #display grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d661208e-1e72-47c5-b751-900253531821",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dg\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dg' is not defined"
     ]
    }
   ],
   "source": [
    "dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5994f3ee-ed0a-4c74-b385-6002e0c27480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth=100, n_estimators=1000)\n",
    "gbc = GradientBoostingClassifier(max_depth= 10, n_estimators= 1000)\n",
    "hgbc = HistGradientBoostingClassifier(learning_rate= 0.01, max_iter= 100)\n",
    "etc = ExtraTreesClassifier(max_depth= None, n_estimators= 1000)\n",
    "bc = BaggingClassifier(n_estimators= 10, random_state= 42)\n",
    "    # ce.CatBoostEncoder(),\n",
    "    # RandomTreesEmbedding(),\n",
    "# rvc = RidgeCV(alphas=1.0) # estimator vs classifier\n",
    "# lvc = LassoCV(eps= 0.001, max_iter= 100, n_alphas= 100)\n",
    "    # SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a62a971-a75f-4485-937f-b314d4974c97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78572c48-30b4-4174-8036-e92c561909a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The least populated class in y has only 2 members, which is less than n_splits=5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99 (+/- 0.00) [RandomForestClassifier]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The least populated class in y has only 2 members, which is less than n_splits=5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00 (+/- 0.00) [GradientBoostingClassifier]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The least populated class in y has only 2 members, which is less than n_splits=5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99 (+/- 0.00) [HistGradientBoostingClassifier]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The least populated class in y has only 2 members, which is less than n_splits=5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99 (+/- 0.00) [ExtraTreesClassifier]\n",
      "Accuracy: 1.00 (+/- 0.00) [BaggingClassifier]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "The least populated class in y has only 2 members, which is less than n_splits=5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00 (+/- 0.00) [Voting]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "Using 4584 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3832435042424d9731eec6144f60ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n",
      "X does not have valid feature names, but BaggingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mKernelExplainer(vc\u001b[38;5;241m.\u001b[39mpredict_proba, X_train) \n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Compute SHAP values for test set \u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(X_test) \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Plot summary plot of SHAP values for each feature \u001b[39;00m\n\u001b[1;32m     42\u001b[0m shap\u001b[38;5;241m.\u001b[39msummary_plot(shap_values[\u001b[38;5;241m1\u001b[39m], X_test)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/shap/explainers/_kernel.py:242\u001b[0m, in \u001b[0;36mKernel.shap_values\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index:\n\u001b[1;32m    241\u001b[0m     data \u001b[38;5;241m=\u001b[39m convert_to_instance_with_index(data, column_name, index_value[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], index_name)\n\u001b[0;32m--> 242\u001b[0m explanations\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplain(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgc_collect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    244\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/shap/explainers/_kernel.py:436\u001b[0m, in \u001b[0;36mKernel.explain\u001b[0;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernelWeights[nfixed_samples:] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m weight_left \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernelWeights[nfixed_samples:]\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# execute the model on the synthetic samples we have created\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# solve then expand the feature importance (Shapley value) vector to contain the non-varying features\u001b[39;00m\n\u001b[1;32m    439\u001b[0m phi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mgroups_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/shap/explainers/_kernel.py:575\u001b[0m, in \u001b[0;36mKernel.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index_ordered:\n\u001b[1;32m    574\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msort_index()\n\u001b[0;32m--> 575\u001b[0m modelOut \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mf(data)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modelOut, (pd\u001b[38;5;241m.\u001b[39mDataFrame, pd\u001b[38;5;241m.\u001b[39mSeries)):\n\u001b[1;32m    577\u001b[0m     modelOut \u001b[38;5;241m=\u001b[39m modelOut\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_voting.py:409\u001b[0m, in \u001b[0;36mVotingClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute probabilities of possible outcomes for samples in X.\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;124;03m    Weighted average probability for each class per sample.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    407\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    408\u001b[0m avg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collect_probas(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weights_not_none\n\u001b[1;32m    410\u001b[0m )\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m avg\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_voting.py:384\u001b[0m, in \u001b[0;36mVotingClassifier._collect_probas\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_collect_probas\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([clf\u001b[38;5;241m.\u001b[39mpredict_proba(X) \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_voting.py:384\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_collect_probas\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([clf\u001b[38;5;241m.\u001b[39mpredict_proba(X) \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:885\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    880\u001b[0m all_proba \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    881\u001b[0m     np\u001b[38;5;241m.\u001b[39mzeros((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], j), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_)\n\u001b[1;32m    883\u001b[0m ]\n\u001b[1;32m    884\u001b[0m lock \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n\u001b[0;32m--> 885\u001b[0m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, require\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msharedmem\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\n\u001b[1;32m    886\u001b[0m     delayed(_accumulate_prediction)(e\u001b[38;5;241m.\u001b[39mpredict_proba, X, all_proba, lock)\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\n\u001b[1;32m    888\u001b[0m )\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m proba \u001b[38;5;129;01min\u001b[39;00m all_proba:\n\u001b[1;32m    891\u001b[0m     proba \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:664\u001b[0m, in \u001b[0;36m_accumulate_prediction\u001b[0;34m(predict, X, out, lock)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_accumulate_prediction\u001b[39m(predict, X, out, lock):\n\u001b[1;32m    658\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;124;03m    This is a utility function for joblib's Parallel.\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m    It can't go locally in ForestClassifier or ForestRegressor, because joblib\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;124;03m    complains that it cannot pickle it when placed there.\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m predict(X, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[1;32m    666\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/tree/_classes.py:1003\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.predict_proba\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m   1001\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1002\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_X_predict(X, check_input)\n\u001b[0;32m-> 1003\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1006\u001b[0m     proba \u001b[38;5;241m=\u001b[39m proba[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import shap \n",
    "from sklearn.ensemble import  VotingClassifier \n",
    "from sklearn.model_selection import cross_val_score\n",
    "X = m2_pipeline.drop(columns=['label'])\n",
    "y = m2_pipeline['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Define VotingClassifier \n",
    "vc = VotingClassifier(estimators=[('rfc',rfc),('gbc',gbc),('hgbc',hgbc),('etc',etc),('bc',bc)], voting='soft') \n",
    "rfc.fit(X_train,y_train)\n",
    "gbc.fit(X_train,y_train)\n",
    "hgbc.fit(X_train,y_train)\n",
    "etc.fit(X_train,y_train)\n",
    "bc.fit(X_train,y_train)\n",
    "# rvc.fit(X_train,y_train)\n",
    "# lvc.fit(X_train,y_train)\n",
    "vc.fit(X_train,y_train)\n",
    "# runnable = [('rfc',rfc),('gbc',gbc),('hgbc',hgbc),('etc',etc),('bc',bc),('rvc',rvc),('lvc',lvc),('ensemble', vc)]\n",
    "# #fit all\n",
    "for clf, label in zip([rfc,gbc,hgbc,etc,bc,vc], ['RandomForestClassifier', 'GradientBoostingClassifier', 'HistGradientBoostingClassifier', 'ExtraTreesClassifier',\\\n",
    "                                                    'BaggingClassifier','Voting']):\n",
    "    scores = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=5)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "explainer = shap.KernelExplainer(vc.predict_proba, X_train) # Define explainer\n",
    "shap_values = explainer.shap_values(X_test) # Compute SHAP values for test set \n",
    "shap.summary_plot(shap_values[1], X_test) # Plot summary plot of SHAP values for each feature "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
