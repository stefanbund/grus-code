{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aaa4fcc-1a03-4f20-a9dd-7ce3d60ccb52",
   "metadata": {},
   "source": [
    "## premise: Grid search for clustering solution\n",
    "\n",
    "via skleanr pipeline and grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beea8fea-ae81-4e3f-9bff-fc4f8bd3f443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0084e4ae-20f9-4081-93df-7822264899b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "precursor_buy_cap_pct_change    float64\n",
       "precursor_ask_cap_pct_change    float64\n",
       "precursor_bid_vol_pct_change    float64\n",
       "precursor_ask_vol_pct_change    float64\n",
       "change.1                        float64\n",
       "surge_targets_met_pct           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load pipeline 1 csv and prep for clustering\n",
    "m2_pipeline = pd.read_csv('pipeline1.csv')\n",
    "#change is surge price rate of change per observation, change.1 is precursor\n",
    "#sum_change is surge sum_change per surge, and surge_area is surge alone\n",
    "keepable = ['precursor_buy_cap_pct_change', \n",
    "            'precursor_ask_cap_pct_change',\n",
    "            'precursor_bid_vol_pct_change', \n",
    "            'precursor_ask_vol_pct_change', 'change.1',\n",
    "            'surge_targets_met_pct']\n",
    "\n",
    "# Normalize the 'surge_targets_met_pct' column\n",
    "x = m2_pipeline[['surge_targets_met_pct']].values.astype(float)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "m2_pipeline['surge_targets_met_pct_normalized'] = pd.DataFrame(x_scaled)\n",
    "\n",
    "m2_pipeline = m2_pipeline[keepable]\n",
    "m2_pipeline = m2_pipeline.dropna()\n",
    "print(m2_pipeline.isna().sum(axis=1).astype(bool).sum())\n",
    "m2_pipeline = m2_pipeline.astype('float')\n",
    "m2_pipeline.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d454ab-b649-4abd-bce4-801893ab1ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sklearn.metrics.get_scorer_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "150bccbc-b02a-4d47-829a-09e13fbc488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IF you must cluster on the column 'surge_targets_met_pct' like so\n",
    "# import pandas as pd\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# # Assuming your dataframe is named 'm2_pipeline'\n",
    "# kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "# kmeans.fit(m2_pipeline[['surge_targets_met_pct_normalized']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c089cf4d-7ce1-4e9d-90cc-200e5e874716",
   "metadata": {},
   "source": [
    "### clustering types\n",
    "\n",
    "K-Means\n",
    "Affinity propagation\n",
    "Mean Shift\n",
    "Spectral Clustering\n",
    "Agglomerative Clustering\n",
    "DBSCAN\n",
    "OPTICS\n",
    "Birch\n",
    "Gaussian Mixture\n",
    "MiniBatch K-Means\n",
    "Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432f579-c5e4-4cfb-b90d-5208ebd28da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for cluster size 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'KMeans', 'score': 0.6468266676607601, 'clusters': 4}\n"
     ]
    }
   ],
   "source": [
    "## alternative: specifying cluster value\n",
    "import pandas as pd \n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering,\\\n",
    "MeanShift, AffinityPropagation, DBSCAN, OPTICS, Birch\n",
    "from sklearn.metrics import silhouette_score \n",
    "\n",
    "data =  m2_pipeline\n",
    "scores = []  # feed in experiments as a dictionary of parameters and scores\n",
    "\n",
    "csize = [4,5,6,7,8,9,10, 11, 12]\n",
    "for i in range(len(csize)):\n",
    "    c= csize[i]\n",
    "    print(\"for cluster size\",c)\n",
    "    kmeans = KMeans(n_clusters=c) #\n",
    "    hierarchical = AgglomerativeClustering(n_clusters=c) \n",
    "    spectral = SpectralClustering(n_clusters=c) \n",
    "    birch = Birch(n_clusters=c)\n",
    "    algorithms = [(kmeans, 'KMeans'), (hierarchical, 'Agglomerative'), (spectral, 'Spectral'), (birch, 'Birch')] \n",
    "    # algorithms = [kmeans, hierarchical, spectral, birch] \n",
    "\n",
    "    # for algorithm, name in algorithms: # Loop over the algorithms and calculate the silhouette score for each\n",
    "    for algorithm, name in algorithms: # Loop over the algorithms and calculate the silhouette score for each\n",
    "        algorithm.fit(data) \n",
    "        score= silhouette_score(data, algorithm.labels_) \n",
    "        fin = {\"algo\":name, \"score\":score, \"clusters\":c}\n",
    "        print(fin)\n",
    "        scores.append(fin) \n",
    "\n",
    "results = pd.DataFrame(scores)  #{'Algorithm': [name for _, name in algorithms], 'Silhouette Score': scores}) \n",
    "print(results) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ac6ec6-07ca-4446-a171-7514528aae03",
   "metadata": {},
   "source": [
    "## do non cluster specified clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b488d-2a38-4a29-9db2-0ff80367ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanshift = MeanShift(*, bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300)\n",
    "affinity = AffinityPropagation(*, damping=0.5, max_iter=200, convergence_iter=15, copy=True, preference=None, affinity='euclidean', verbose=False, random_state=None)\n",
    "dbscan = DBSCAN(eps=0.5, *, min_samples=5, metric='euclidean', metric_params=None, algorithm='auto', leaf_size=30, p=None, n_jobs=None)\n",
    "optics = OPTICS(*, min_samples=5, max_eps=inf, metric='minkowski', p=2, metric_params=None, cluster_method='xi', eps=None, xi=0.05, predecessor_correction=True, min_cluster_size=None, algorithm='auto', leaf_size=30, memory=None, n_jobs=None)\n",
    "\n",
    "\n",
    "data = m2_pipeline\n",
    "# scores2 = []  # feed in experiments as a dictionary of parameters and scores\n",
    "algorithms2 = [(meanshift, 'Meanshift'), (affinity, 'Affinity'), (dbscan, 'DBSCAN'), (optics, 'OPTICS')]\n",
    "for algorithm, name in algorithms2: # Loop over the algorithms and calculate the silhouette score for each\n",
    "        algorithm.fit(data) \n",
    "        score= silhouette_score(data, algorithm.labels_) \n",
    "        fin = {\"algo\":name, \"score\":score, \"clusters\":0} \n",
    "        print(fin)\n",
    "        scores.append(fin) \n",
    "# Create a dataframe to store the results \n",
    "results = pd.DataFrame(scores) #{'Algorithm': [name for _, name in algorithms2], 'Silhouette Score': scores2}) \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c956a-97b0-4aea-ab44-bb91c5861e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = 'ranked_classifier_search.csv'  #clustered methods, specified num clusters\n",
    "results.to_csv(cm, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24043d0-1bb8-4920-8bd2-1443fc526324",
   "metadata": {},
   "source": [
    "## revive the test from stored memory\n",
    "via csv, then get top n, n=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e1de45-14ca-42e8-9f87-c25f49cb418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "revive = pd.read_csv(cm)\n",
    "print(revive.head(2))\n",
    "print(revive.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f88d16b-5d01-4f3a-a230-af262364ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = revive.sort_values('score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d05c5-9316-4b1e-964f-3aae0bdc43e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5588d8-7f9a-4db4-981c-08bbf65a7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topn.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9438c5-46cc-4392-884e-3039e9845cf5",
   "metadata": {},
   "source": [
    "## cluster and back test the cluster as trade strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26324684-6a33-4962-a32b-73a46e4b6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cluster range evaluation\n",
    "# Bin the values in the 'surge_targets_met_pct' column after grouped by 'cluster'\n",
    "# bins = [-float('inf'), -8.5, -5.64, 0, 0.25, 0.35, 0.4, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0, 5.0, 5.64, float('inf')]\n",
    "# labels = ['< -8.5', '-8.5 to -5.64', '-5.64 to 0', '0 to 0.25', '0.25 to 0.35', '0.35 to 0.4', '0.4 to 0.5', '0.5 to 0.75', '0.75 to 1', '1 to 2', '2 to 3', '3 to 4', '4 to 5', '5 to 5.64', '> 5.64']\n",
    "# lc['binned'] = pd.cut(lc['surge_targets_met_pct'], bins=bins, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380098e5-f13d-4bed-ba94-0d4392fed1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "scoring = [] #algo, cluster quality metrics, cluster sizes, from topn\n",
    "for index, row in topn.iterrows():\n",
    "    data = m2_pipeline #need a temporary version of the master data\n",
    "\n",
    "    algo = row['algo']   # Extract the algorithm and cluster from the current row\n",
    "    c = row['clusters']\n",
    "    score = row['score']\n",
    "    kmeans = KMeans(n_clusters=c) #establish a means to build the cluster algo, cluster or no NEED SEEDING \n",
    "    hierarchical = AgglomerativeClustering(n_clusters=c) \n",
    "    spectral = SpectralClustering(n_clusters=c) \n",
    "    birch = Birch(n_clusters=c)\n",
    "    meanshift = MeanShift()\n",
    "    affinity = AffinityPropagation()\n",
    "    dbscan = DBSCAN()\n",
    "    optics = OPTICS()\n",
    "    algorithms = [(kmeans, 'KMeans'), (hierarchical, 'Agglomerative'), (spectral, 'Spectral'), (birch, 'Birch')\\\n",
    "                  ,(meanshift, 'Meanshift'), (affinity, 'Affinity'), (dbscan, 'DBSCAN'), (optics, 'OPTICS')]\n",
    "   \n",
    "    method = [t for t in algorithms if algo in t][0][0] #read the algo type from the topn list/df\n",
    "    print(\"handle: \",algo, c, score, method) #verify it is reading it ok\n",
    "    # method.fit(m2_pipeline)\n",
    "    predict = method.fit_predict(data) #fit predict, derive cluster labeling\n",
    "    data['cluster'] = pd.Series(predict, index=data.index) # Add a new column to the filtered dataframe with the predicted cluster labels\n",
    "    # group by cluster identifier, define ranges of values, number of clusters and their ranged values\n",
    "    #NEED A NEW ALGORITHM TO EVALUATE EACH CLUSTER, BY BUSINESS RULE\n",
    "    # means = data.groupby('Cluster')['surge_targets_met_pct'].mean() ##group by then aggregate\n",
    "    min_max_count = data.groupby('cluster')['surge_targets_met_pct'].agg(['min', 'max', 'count'])\n",
    "    print(type(min_max_count))\n",
    "#cluster analytics: \n",
    "    \n",
    "    fin = {\"algo\":method, \"score\":score, \"clusters\":c, \"metrics\":min_max_count}\n",
    "    print(fin)\n",
    "    scoring.append(fin)\n",
    "\n",
    "analytic = pd.DataFrame(scoring)\n",
    "print(analytic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7359847b-77b2-4b04-a338-cd54f4baf9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "analytic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5bb340-bd36-4621-a2d8-2dc47dd03cca",
   "metadata": {},
   "source": [
    "### take top performing cluster techniques, then ...\n",
    "\n",
    "attache labels to each and analyze the clustering profiles for efficiency, profit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec91dc-e6a0-4194-a7fe-e7c04cd31b42",
   "metadata": {},
   "source": [
    "## notes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed17c724-9a6c-4280-a565-b4a9f9904db8",
   "metadata": {},
   "source": [
    "1. grid search notes on [sklearn](https://scikit-learn.org/stable/modules/grid_search.html#grid-search)\n",
    "2. sklearn pipeline [guide](https://scikit-learn.org/stable/modules/compose.html#pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
