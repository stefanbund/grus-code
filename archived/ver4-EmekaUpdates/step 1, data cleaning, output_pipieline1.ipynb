{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: merge all .csv files into a one-year dataframe\n",
    "\n",
    "loop through the lob_caps directory, forming one time-sorted dataframe, with all CAPS files. These files captured sample bid and ask capitalization, and respective bid and ask volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting altair\n",
      "  Obtaining dependency information for altair from https://files.pythonhosted.org/packages/f2/b4/02a0221bd1da91f6e6acdf0525528db24b4b326a670a9048da474dfe0667/altair-5.1.1-py3-none-any.whl.metadata\n",
      "  Downloading altair-5.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from altair) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from altair) (4.17.3)\n",
      "Requirement already satisfied: numpy in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from altair) (1.24.3)\n",
      "Requirement already satisfied: packaging in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from altair) (23.0)\n",
      "Requirement already satisfied: pandas>=0.25 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from altair) (1.5.3)\n",
      "Requirement already satisfied: toolz in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from altair) (0.12.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair) (0.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from pandas>=0.25->altair) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from pandas>=0.25->altair) (2022.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from jinja2->altair) (2.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/stefanbund/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas>=0.25->altair) (1.16.0)\n",
      "Downloading altair-5.1.1-py3-none-any.whl (520 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.6/520.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: altair\n",
      "Successfully installed altair-5.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install matplotlib\n",
    "!pip3 install altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: mv [-f | -i | -n] [-hv] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n"
     ]
    }
   ],
   "source": [
    "!mv $(find . -type d -name \"lob_caps\" -exec grep -q MATCH {} \\; -print0 | xargs -0 echo) backup_match/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m     li\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[0;32m---> 20\u001b[0m capsFrame \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(li, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m#end frame contains all data\u001b[39;00m\n\u001b[1;32m     21\u001b[0m capsFrame\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)   \u001b[38;5;66;03m#sorted by time into one time series\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor new df: \u001b[39m\u001b[38;5;124m\"\u001b[39m, capsFrame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[1;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    369\u001b[0m         objs,\n\u001b[1;32m    370\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m    371\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[1;32m    372\u001b[0m         join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[1;32m    373\u001b[0m         keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[1;32m    374\u001b[0m         levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[1;32m    375\u001b[0m         names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[1;32m    376\u001b[0m         verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[1;32m    377\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    378\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/concat.py:425\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    422\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/a/21232849 model \n",
    "def getCAPSByDateAndType(type):  #returns a dict, date + df caps for that date, then extended date and time\n",
    "                                # print(\"for type, \", type)  ./lob_caps/\n",
    "    ret = []\n",
    "    for root, dirs, files in os.walk(\"./lob_caps\"): #core/gh-code/grus-code/ver2-pctChangeDriven/lob_caps\n",
    "        for filename in files:\n",
    "            if type in filename:\n",
    "                # print(\"CAPS file, \", filename) #mac, do find . -name ._\\* -delete\n",
    "                ret.append(filename)\n",
    "    return ret\n",
    "\n",
    "csvFileList = getCAPSByDateAndType(\"CAPS\") #iterate this array to dip into each csv, later on\n",
    "li = []                         #form the endFrame / global data frame around this array\n",
    "for filename in csvFileList:\n",
    "    csv = \"lob_caps/\" + filename\n",
    "    # print(csv)\n",
    "    df = pd.read_csv(csv, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "capsFrame = pd.concat(li, axis=0, ignore_index=True) #end frame contains all data\n",
    "capsFrame.sort_values(by=['time'], ascending=True)   #sorted by time into one time series\n",
    "print(\"for new df: \", capsFrame.shape[0])\n",
    "start = capsFrame[\"time\"].min()\n",
    "end = capsFrame[\"time\"].max()\n",
    "print(\"start: \", start, \" end: \", end)\n",
    "print(capsFrame.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capsFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schema for capitalization data\n",
    "\n",
    "loads the csv files, as acquired from coinbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capsFrame.head(2) #shows the basic data collection via coinbase, these are aggregated values, collected several x a minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values with last non-null value\n",
    "capsFrame['bc'] = capsFrame['bc'].fillna(method='ffill')\n",
    "capsFrame['ac'] = capsFrame['ac'].fillna(method='ffill')\n",
    "capsFrame['tbv'] = capsFrame['tbv'].fillna(method='ffill')\n",
    "capsFrame['tav'] = capsFrame['tav'].fillna(method='ffill')\n",
    "capsFrame['mp'] = capsFrame['mp'].fillna(method='ffill')\n",
    "capsFrame['minBid'] = capsFrame['minBid'].fillna(method='ffill')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover precursor and surge episodes\n",
    "\n",
    "the goal of the data prep is to discover periods of continuous, positive momentum. These are **surges**. \n",
    "\n",
    "The periods preceding surges are, for the sake of the experiment, **precursors**. They are detected as periods of discontinuous positive momentum, or negative momentum. \n",
    "\n",
    "A ten-row window is used to calculate positive or negative momentum. A percent **change** is calculated for the ten row subsample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regularization of critical features\n",
    "get percent change as basis for comprehending LOB\n",
    "\n",
    "create new columns which depict the momentum of one row versus the next, in terms of price , capitalization and volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your time series data into a pandas dataframe\n",
    "# consider cahnging this approach because it doesnt actually check in between values\n",
    "\n",
    "caps_df = capsFrame   \n",
    "lookback_period = 10 # in rows\n",
    "caps_df['change'] = caps_df['mp'].pct_change(periods=lookback_period)\n",
    "caps_df['bc_change'] = caps_df['bc'].pct_change(periods=lookback_period)\n",
    "caps_df['ac_change'] = caps_df['ac'].pct_change(periods=lookback_period)\n",
    "caps_df['tav_change'] = caps_df['tav'].pct_change(periods=lookback_period)\n",
    "caps_df['tbv_change'] = caps_df['tbv'].pct_change(periods=lookback_period)\n",
    "## key components: bc_change, ac_change, tav_change, tbv_change, change\n",
    "# caps_df.sample\n",
    "print(caps_df.shape[0], caps_df.columns)# Calculate the returns of your asset over a fixed lookback period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  establish benchmarks for percent change\n",
    "\n",
    "the mean of change represents the average rate of change between LOB samples. This is used to determine whether the change between rows is significant or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for period, average or mean change metric. this changes with window size\n",
    "meanChange = round(caps_df['change'].mean(),8)\n",
    "meanChange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data mining: sequence discovery\n",
    "define precursors from surges, prepare the data with this sequence: \n",
    "\n",
    "precursor -> surge\n",
    "\n",
    "prepare to cluster every precursor, by the sequential, resultant surge. Do not assume causality, but rather preoccurance.\n",
    "\n",
    "use the threshold, mean change as tool to separate precursor from surges, where surges represent periods of positive momentum above threshold.\n",
    "\n",
    "This step defines the data schema for the remainder of the process, where key statistics are defined for precursors and surges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify units of 10 rows where the percent change is greater or less than the threshold\n",
    "### key components: bc_change, ac_change, tav_change, tbv_change, change\n",
    "threshold = meanChange\n",
    "surges = []\n",
    "precursors = []\n",
    "for i in range(0,len(caps_df),10):\n",
    "    if caps_df.iloc[i:i+10]['change'].mean() >= threshold:\n",
    "        surges.append({'time': caps_df.iloc[i]['time'],\n",
    "                       's_MP': caps_df.iloc[i]['mp'],\n",
    "                       'change': caps_df.iloc[i:i+10]['change'].mean(),\n",
    "                       'type':'surge'})  #['bc', 'ac', 'tbv', 'tav', 'time', 'mp', 'minBid', 'change']\n",
    "    else:\n",
    "        precursors.append({'time': caps_df.iloc[i]['time'],\n",
    "                           'p_MP': caps_df.iloc[i]['mp'],\n",
    "                           'change': caps_df.iloc[i:i+10]['change'].mean(),\n",
    "                            'type':'precursor',\n",
    "                            'precursor_buy_cap_pct_change':caps_df.iloc[i]['bc_change'], \n",
    "                            'precursor_ask_cap_pct_change':caps_df.iloc[i]['ac_change'],\n",
    "                            'precursor_bid_vol_pct_change':caps_df.iloc[i]['tbv_change'],\n",
    "                            'precursor_ask_vol_pct_change':caps_df.iloc[i]['tav_change']\n",
    "                            })  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for item in surges[:2]:\n",
    "    #print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for item in precursors:\n",
    "    #print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepprocess: merge precursors and surges into time series\n",
    "\n",
    "a dataframe of sequences, **sequence_df** is created by concatenating both buckets, and sorting by time. This will create a time series of surge and precursor periods, as defined by: \n",
    "\n",
    "* 10 window percent change values\n",
    "* contiguity: these precursor and surges are next to each other and thus have a length or duration of momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surges_df = pd.DataFrame(surges)\n",
    "precursors_df = pd.DataFrame(precursors)\n",
    "sequence_df = pd.concat([surges_df, precursors_df]).sort_values(by=['time'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### view the aligned, continuous time series of precursors and surges\n",
    "\n",
    "view the final abstraction: sets of precursor periods, next to surges, in a linear time series. Precursors effectively precede surges on a linear time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in sequence_df.iterrows():\n",
    "#     print(row['surge'], row['precursor'])\n",
    "sequence_df['type'].head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_df.head(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize proof of algorithmic accuracy\n",
    "\n",
    "this chart will plot the price time series, with an area of precursor and surge, as proof of our algorithmic accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = sequence_df[:4999]\n",
    "line = alt.Chart(subset).mark_line(color='green').encode(\n",
    "    x='time',\n",
    "    y='s_MP'\n",
    ")\n",
    "\n",
    "s_bar = alt.Chart(subset).mark_bar().encode(\n",
    "    x='time',\n",
    "    y='s_MP',\n",
    "    color='type:N'\n",
    ")\n",
    "\n",
    "p_bar = alt.Chart(subset).mark_bar().encode(\n",
    "    x='time',\n",
    "    y='p_MP',\n",
    "    color='type:N'\n",
    ")\n",
    "\n",
    "chart = (s_bar + p_bar + line).properties(width=600, height=500)\n",
    "chart.title = 'Data Mining Accuracy, Surge vs Precursor Sequence'\n",
    "subtitle = 'Precursors are contiguous periods where percentage rate of growth is less than threshold'\n",
    "chart.properties(title=alt.TitleParams(text=[chart.title, subtitle], baseline='bottom', orient='top', anchor='start', fontSize=14))\n",
    "chart.interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data mining 2: information gain, create new features\n",
    "\n",
    "Perform information gain on grouped precursors and surges\n",
    "\n",
    "define the **sum change**, or total change per continuous episode (precursor or surge). \n",
    "\n",
    "define the **length** of each episode. \n",
    "\n",
    "define the height of the surge, how high did the continuous positive momentum reach?\n",
    "\n",
    "define the size (area) of the surge, as a triangular area (height times length), as **surge_area**\n",
    "\n",
    "Create one line to describe a precursor or search and it's related order book statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_df['group'] = (sequence_df['type'] != sequence_df['type'].shift(1)).cumsum()\n",
    "columns_to_transform = [\n",
    "    'precursor_buy_cap_pct_change',\n",
    "    'precursor_ask_cap_pct_change',\n",
    "    'precursor_bid_vol_pct_change',\n",
    "    'precursor_ask_vol_pct_change'\n",
    "]\n",
    "\n",
    "for col in columns_to_transform:\n",
    "    sequence_df[col] = sequence_df.groupby('group')[col].transform(lambda x: x.sum() if not x.isna().all() else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # impute missing values with last non-null value DONE PRIOR, NOW AT START\n",
    "sequence_df['s_MP'] = sequence_df['s_MP'].fillna(method='ffill')\n",
    "sequence_df['p_MP'] = sequence_df['p_MP'].fillna(method='ffill')\n",
    "sequence_df['precursor_buy_cap_pct_change'] = sequence_df['precursor_buy_cap_pct_change'].fillna(method='ffill')\n",
    "sequence_df['precursor_ask_cap_pct_change'] = sequence_df['precursor_ask_cap_pct_change'].fillna(method='ffill')\n",
    "sequence_df['precursor_bid_vol_pct_change'] = sequence_df['precursor_bid_vol_pct_change'].fillna(method='ffill')\n",
    "sequence_df['precursor_ask_vol_pct_change'] = sequence_df['precursor_ask_vol_pct_change'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence_df['group'] = (sequence_df['type'] != sequence_df['type'].shift(1)).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_df['length'] = sequence_df.groupby(['type', 'group'])['group'].transform('count')\n",
    "\n",
    "print(sequence_df.shape[0])\n",
    "sequence_df['sum_change'] = sequence_df.groupby(['type', 'group'])['change'].transform('sum')\n",
    "\n",
    "sequence_df['max_surge_mp'] = sequence_df.groupby(['type', 'group'])['s_MP'].transform('max')\n",
    "sequence_df['min_surge_mp'] = sequence_df.groupby(['type', 'group'])['s_MP'].transform('min')\n",
    "\n",
    "sequence_df['max_precursor_mp'] = sequence_df.groupby(['type', 'group'])['p_MP'].transform('max')\n",
    "sequence_df['min_precursor_mp'] = sequence_df.groupby(['type', 'group'])['p_MP'].transform('min')\n",
    "\n",
    "sequence_df['area']  = sequence_df.apply(lambda row: row['length'] * row['sum_change'], axis=1)\n",
    "\n",
    "sequence_df.loc[sequence_df['type'] == 'surge', 'surge_area'] = sequence_df.loc[sequence_df['type'] == 'surge', 'area']\n",
    "\n",
    "sequence_df['surge_targets_met_pct']  = sequence_df.apply(lambda group: ((group['max_precursor_mp']-group['max_surge_mp'])/group['max_surge_mp']  ) *100, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom function to calculate the percentage by which max_surge_mp exceeds max_precursor_mp\n",
    "'''for a pandas dataframe wth attributes ['group', 'time', 's_MP', 'change', 'type', 'length', 'sum_change',\n",
    "       'max_surge_mp', 'min_surge_mp', 'area', 'surge_area', 'group', 'time',\n",
    "       'change', 'type', 'p_MP', 'precursor_buy_cap_pct_change',\n",
    "       'precursor_ask_cap_pct_change', 'precursor_bid_vol_pct_change',\n",
    "       'precursor_ask_vol_pct_change', 'length', 'sum_change',\n",
    "       'max_precursor_mp', 'min_precursor_mp', 'area'] \n",
    "       group by type, group then create  \n",
    "       a new column 'surge_targets_met_pct' which equals the percentage \n",
    "       by which the max_surge_mp exceeds the max_precursor_mp'''\n",
    "\n",
    "\n",
    "sequence_df.columns\n",
    "print(sequence_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data mining 3: form final sequences by statistical weight\n",
    "\n",
    "Critical group by unique identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df = sequence_df.groupby('group').first().reset_index()\n",
    "# print(unique_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge even and odd Rows to form the final sequences\n",
    "\n",
    "Even rows contain surge, and odd rows contain precursors. **When you merge them, you form a sequence of precursor, and surge.**\n",
    "\n",
    "Each row will contain a continuous **precursor->surge** sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to start with a precursor removes the first surge\n",
    "unique_df = unique_df.iloc[1:]\n",
    "even_df = unique_df.iloc[::2].reset_index(drop=True)\n",
    "odd_df = unique_df.iloc[1::2].reset_index(drop=True)\n",
    "\n",
    "merged_df = pd.concat([even_df, odd_df], axis=1)\n",
    "\n",
    "# print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_cols = merged_df.dropna(axis=1, how='all')\n",
    "nan_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_cols.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV: step one, pipeline\n",
    "Label to use is surge_targets_met_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nan_cols.to_csv('pipeline1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
