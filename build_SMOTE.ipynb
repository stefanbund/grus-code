{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## expand a labeled dataset to SMOTE\n",
    "\n",
    "magnify the number of minority/exceptional cases within the sequence dataset, ideally targets the binary binned dataset.\n",
    "\n",
    "[reference 1](<ver5-ordinal-binning-grid-searches/step 2-0, ranged clustering, with time.ipynb>)\n",
    "\n",
    "different oversampling tools: Naive random oversampling, SMOTE, ADASYN, SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from collections import Counter\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from imblearn.ensemble import BalancedBaggingClassifier,BalancedRandomForestClassifier,RUSBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_pipeline = pd.read_csv(\"binary_binned_pipeline1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_pipeline.columns #do this to identify the index of the categorical feature, for below setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up toolsets as functions to build separate datasets, bin_Naive, bin_SMOTE, bin_ADASYN\n",
    "\n",
    "def build_naive(d):  #https://imbalanced-learn.org/stable/over_sampling.html#naive-random-over-sampling\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    print(\"ROS\",sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def build_smote(d): #https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html#smote\n",
    "    X_resampled, y_resampled = SMOTE(random_state=42,categorical_features='label', categorical_encoder=None, ).fit_resample(X, y)\n",
    "    print(\"SMOTE\",sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def build_adasyn(d): #https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.ADASYN.html#adasyn\n",
    "    X_resampled, y_resampled = ADASYN(random_state=42).fit_resample(X, y)\n",
    "    print(\"ADASYN\",sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def build_borderline(d): #https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.BorderlineSMOTE.html#borderlinesmote\n",
    "    X_resampled, y_resampled = BorderlineSMOTE(random_state=42).fit_resample(X, y)\n",
    "    print(\"BORDERLINE\",sorted(Counter(y_resampled).items())) \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def build_smotenc(d): #https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.html#smotenc\n",
    "    smote_nc = SMOTENC(categorical_features=[0, 2], random_state=42)\n",
    "    X_resampled, y_resampled = smote_nc.fit_resample(X, y)\n",
    "    print(\"SMOTENC\",sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def build_svmsmote(d):\n",
    "    sm = SVMSMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "    print(\"SVMSMOTE\",'Resampled dataset shape %s' % Counter(y_res))\n",
    "    return X_res, y_res\n",
    "\n",
    "def build_kmsmote(d): #https://imbalanced-learn.org/stable/combine.html#combination-of-over-and-under-sampling\n",
    "    m = KMeansSMOTE(kmeans_estimator=MiniBatchKMeans(n_init=1, random_state=0), random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "    # Find the number of new samples in the middle blob\n",
    "    n_res_in_middle = ((X_res[:, 0] > -5) & (X_res[:, 0] < 5)).sum()\n",
    "    print(\"KMSMOTE\",\"Samples in the middle blob: %s\" % n_res_in_middle)\n",
    "    return X_res, y_res\n",
    "\n",
    "def build_smoteenn(d):\n",
    "    smote_enn = SMOTEENN(random_state=0)\n",
    "    X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "    print(\"SMOTEENN\",sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    " def build_smotetomek(d):\n",
    "    smote_tomek = SMOTETomek(random_state=0)\n",
    "    X_resampled, y_resampled = smote_tomek.fit_resample(X, y)\n",
    "    print(sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take optimal classifier parameters and technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the dataframe into features and labels\n",
    "# X = m2_pipeline.drop(columns=['label'])\n",
    "getBestClassifier(name, dataset):   #'kmsmote', build_svmsmote(source)\n",
    "    # y = m2_pipeline['label'].values #per https://stackoverflow.com/a/73095562/12001832\n",
    "    # X = m2_pipeline[keepable].values\n",
    "    y = dataset[1].values\n",
    "    X = dataset[0].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()  #normalize all numeric columns\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    classifiers = [  # Define the classifiers and their respective hyperparameters\n",
    "        RandomForestClassifier(),\n",
    "        GradientBoostingClassifier(),\n",
    "        HistGradientBoostingClassifier(),\n",
    "        ExtraTreesClassifier(),\n",
    "        BaggingClassifier(),\n",
    "        RidgeCV(),\n",
    "        LassoCV(),\n",
    "        SVC(),\n",
    "        LogisticRegression(),\n",
    "        NaiveBayes(),\n",
    "        KNNeighbors(),\n",
    "    ]\n",
    "    params = {\n",
    "        'RandomForestClassifier': {'n_estimators': [10, 100, 1000], 'max_depth': [None, 10, 100]},\n",
    "        'GradientBoostingClassifier': {'n_estimators': [10, 100, 1000], 'max_depth': [None, 10, 100]},\n",
    "        'HistGradientBoostingClassifier': {'learning_rate': [0.1, 0.01], 'max_iter': [100, 200]},\n",
    "        'ExtraTreesClassifier':{'n_estimators': [10, 100, 1000], 'max_depth': [None, 10, 100]},\n",
    "        'BaggingClassifier':{ 'n_estimators':[10],  'random_state':[42]},\n",
    "        'RidgeCV':{'alphas':[0.1, 1.0, 10.0]},\n",
    "        'LassoCV':{ 'eps':[0.001, 0.01, .1], 'n_alphas':[100,200],  'max_iter':[100,200,300,1000]},\n",
    "        'SVC':{'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "        'LogisticRegression':{},\n",
    "        'NaiveBayes':{},\n",
    "        'KNNeighbors':{}\n",
    "    }\n",
    "    comparative = []\n",
    "    # Perform the grid search\n",
    "    for clf in classifiers:\n",
    "        name = clf.__class__.__name__\n",
    "        if name in params:\n",
    "            grid_search = GridSearchCV(clf, params[name], cv=5)\n",
    "            grid_search.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "            accuracy = grid_search.score(X_test, y_test)\n",
    "            \n",
    "            dict = {\"classifier\":name, \"best_params\":grid_search.best_params_, \"accuracy\":accuracy}\n",
    "            comparative.append(dict)\n",
    "    return(comparative)\n",
    "    # dg = pd.DataFrame(comparative) #display grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trigger the search mechanism, per SMOTE method\n",
    "\n",
    "#loop through each oversampled dataset, then run the classifier search on that set, outlining each set, before hand\n",
    "\n",
    "#for each set, run the classifier search\n",
    "source = 'binary_binned_pipeline.csv'\n",
    "resultSet = []\n",
    "resultSet.concat(getBestClassifier('naive', build_naive(source)))  #many rows\n",
    "resultSet.concat(getBestClassifier('smote', build_smote(source)))\n",
    "resultSet.concat(getBestClassifier('adasyn', build_adasyn(source)))\n",
    "resultSet.concat(getBestClassifier('borderline', build_borderline(source)))\n",
    "resultSet.concat(getBestClassifier('smotenc', build_smotenc(source)))\n",
    "resultSet.concat(getBestClassifier('svmsmote', build_svmsmote(source)))\n",
    "resultSet.concat(getBestClassifier('kmsmote', build_kmsmote(source)))\n",
    "resultSet.concat(getBestClassifier('smoteenn', build_smoteenn(source)))\n",
    "resultSet.concat(getBestClassifier('smotetomek', build_smotetomek(source)))\n",
    "\n",
    "optimals = pd.DataFrame(resultSet)\n",
    "optimals.groupby(column=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pitfalls of oversampling](https://imbalanced-learn.org/stable/common_pitfalls.html#data-leakage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sampling based ensemble methods\n",
    "\n",
    "score each, can you combine into a voter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced bagger\n",
    "\n",
    "bbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                sampling_strategy='not majority',\n",
    "                                replacement=False,\n",
    "                                random_state=42)\n",
    "bbc.fit(X_train, y_train)\n",
    "y_pred = bbc.predict(X_test)\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balanced tree estimator\n",
    "brf = BalancedRandomForestClassifier(\n",
    "    n_estimators=100, random_state=42, sampling_strategy=\"all\", replacement=True\n",
    ")\n",
    "brf.fit(X_train, y_train)\n",
    "y_pred = brf.predict(X_test)\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rusboost = RUSBoostClassifier(n_estimators=200, algorithm='SAMME.R',\n",
    "                              random_state=42)\n",
    "rusboost.fit(X_train, y_train)\n",
    "y_pred = rusboost.predict(X_test)\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
